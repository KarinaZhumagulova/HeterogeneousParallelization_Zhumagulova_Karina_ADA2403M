# Практическая работа №7: Редукция и префиксная сумма на GPU

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена изучению **параллельных алгоритмов редукции и сканирования (префиксной суммы) на GPU**.

В рамках работы реализованы следующие задачи:

1. **Редукция** — вычисление суммы элементов массива на GPU.
2. **Префиксная сумма** — вычисление накопленной суммы элементов массива на GPU.

Для каждой задачи:

* Реализованы CPU- и GPU-версии.
* Используются **различные типы памяти CUDA**: глобальная и разделяемая (shared memory).
* Проведено сравнение производительности для массивов разного размера.
* Проверена корректность результатов на тестовых данных.

## Цель работы

* Изучить параллельные алгоритмы редукции и сканирования.
* Освоить работу с **CUDA** и различными типами памяти GPU.
* Провести сравнительный анализ производительности CPU и GPU.
* Оптимизировать алгоритмы с использованием shared memory.


## Структура репозитория

```
.
├── task_1.cu  # Реализация редукции (суммирование элементов массива)
├── task_2.cu  # Реализация префиксной суммы (Blelloch Scan)
├── README.md           # Этот файл
```

## Теоретическая часть

### Редукция

Редукция — операция, которая сводит множество элементов к одному значению. В работе реализован **алгоритм параллельной редукции на GPU** с использованием:

* **Параллельного "дерева"** внутри блока.
* **Shared memory** для ускорения доступа.
* Финальная сумма блоков вычисляется на CPU.

### Сканирование (префиксная сумма)

Сканирование — операция, которая вычисляет накопленное значение для каждого элемента массива. Пример:

```
Вход:  [1, 2, 3, 4]
Выход: [1, 3, 6, 10]
```

Реализован **Blelloch Scan** с использованием shared memory для ускорения.

### Типы памяти CUDA

* **Глобальная память** — доступна всем потокам, но медленная.
* **Разделяемая память (shared)** — доступна потокам внутри блока, быстрая.
* **Локальная память** — приватная для каждого потока.

## Практическая часть

### Задание 1: Параллельная редукция (суммирование элементов массива)

#### Описание

Редукция — это операция, которая сводит множество элементов к одному значению (например, сумма, минимум, максимум). В данной задаче была реализована параллельная редукция для вычисления **суммы элементов массива** с использованием **CUDA**.

* **CPU:** последовательное суммирование через `std::accumulate`.
* **GPU:** каждый поток обрабатывает элемент массива, блоки объединяются через разделяемую память (`shared memory`) для ускорения.
* Реализованы два варианта GPU:

  1. **Глобальная память:** базовый алгоритм с копированием данных в разделяемую память внутри блока.
  2. **Разделяемая память (shared memory):** оптимизированная версия, полностью использующая shared memory для внутриблоковой редукции.

Размер блока потоков: 256.

#### Реализация

**Алгоритм:**

1. Копирование данных из глобальной памяти в shared memory.
2. Параллельная редукция внутри блока по схеме "дерево".
3. Поток 0 каждого блока записывает частичную сумму блока в глобальную память.
4. Финальная сумма всех блоков вычисляется на CPU.

#### Результаты тестирования

| N       | CPU time (ms) | GPU time (global) (ms) | GPU time (shared) (ms) | Check (Global/Shared) | CPU Sum  | GPU Global Sum | GPU Shared Sum |
| ------- | ------------- | ---------------------- | ---------------------- | --------------------- | -------- | -------------- | -------------- |
| 10      | 0.000533      | 0.125152               | 0.035872               | CORRECT / CORRECT     | 116      | 116            | 116            |
| 100     | 0.001821      | 0.023616               | 0.028704               | CORRECT / CORRECT     | 1101     | 1101           | 1101           |
| 1000    | 0.010601      | 0.022688               | 0.020448               | CORRECT / CORRECT     | 12739    | 12739          | 12739          |
| 10000   | 0.101361      | 0.020384               | 0.017952               | CORRECT / CORRECT     | 125491   | 125491         | 125491         |
| 100000  | 1.025463      | 0.041760               | 0.035424               | CORRECT / CORRECT     | 1249397  | 1249397        | 1249397        |
| 1000000 | 10.422285     | 0.163392               | 0.160160               | CORRECT / CORRECT     | 12499067 | 12499067       | 12499067       |

#### Примеры вывода (N = 10)

```
CPU Sum: 116.000000
GPU Global Sum: 116.000000
GPU Shared Sum: 116.000000
Check: Global=CORRECT, Shared=CORRECT
```

#### Выводы

1. GPU обеспечивает **значительное ускорение** даже для небольших массивов по сравнению с CPU.
2. Использование **shared memory** уменьшает время выполнения внутри блока и повышает производительность.
3. Результаты GPU полностью совпадают с эталонной реализацией на CPU для всех размеров массивов.
4. Работа показала, как правильно применять память CUDA и синхронизацию потоков для эффективной редукции.

### Задание 2: Параллельное сканирование (префиксная сумма)

#### Описание

Сканирование (prefix sum) — это операция, которая вычисляет **накопленную сумму элементов массива**. Например, для массива `[1, 2, 3, 4]` префиксная сумма будет `[1, 3, 6, 10]`.

В данной задаче реализованы **два варианта GPU сканирования** с использованием **CUDA**:

1. **Наивный алгоритм через глобальную память:** каждый поток суммирует все элементы от начала массива до своего индекса.

   * Простой, но неэффективный (сложность O(n²) суммарно).
2. **Алгоритм Blelloch с shared memory:**

   * Использует разделяемую память блока для построения бинарного дерева сумм (up-sweep и down-sweep).
   * После вычисления локальных сканов блоков корректируются с помощью смещений, чтобы получить глобальный результат.
   * Значительно ускоряет выполнение по сравнению с глобальной памятью.

**CPU версия** использует простой последовательный проход по массиву для получения эталонного результата.

Размер блока потоков: 256.

#### Результаты тестирования

| N       | CPU time (ms) | GPU time (global) (ms) | GPU time (shared) (ms) | Check (Global/Shared) | CPU last element | GPU Global last | GPU Shared last |
| ------- | ------------- | ---------------------- | ---------------------- | --------------------- | ---------------- | --------------- | --------------- |
| 10      | 0.000262      | 0.318208               | 0.073728               | CORRECT / CORRECT     | 174              | 174             | 174             |
| 100     | 0.001761      | 0.020992               | 0.043168               | CORRECT / CORRECT     | 1337             | 1337            | 1337            |
| 1000    | 0.009008      | 0.056096               | 0.042048               | CORRECT / CORRECT     | 12999            | 12999           | 12999           |
| 10000   | 0.088470      | 0.454656               | 0.042720               | CORRECT / CORRECT     | 125043           | 125043          | 125043          |
| 100000  | 0.934496      | 17.711905              | 0.082848               | CORRECT / CORRECT     | 1250606          | 1250606         | 1250606         |
| 1000000 | 9.299038      | 778.695679             | 0.240960               | CORRECT / CORRECT     | 12482599         | 12482599        | 12482599        |

#### Пример вывода (N = 10)

```
CPU last element: 174.000000
GPU (global memory) last element: 174.000000
GPU (shared memory) last element: 174.000000
Check: Global=CORRECT, Shared=CORRECT
```

#### Выводы

1. Алгоритм **Blelloch с shared memory** обеспечивает значительное ускорение по сравнению с наивным глобальным подходом и CPU для больших массивов.
2. На маленьких массивах накладные расходы на запуск кернела могут превышать выигрыш от параллельного выполнения.
3. Результаты GPU полностью совпадают с эталонной реализацией на CPU для всех размеров массивов.
4. Работа демонстрирует важность оптимизации памяти CUDA и применения эффективных параллельных алгоритмов для сканирования.


### Контрольные вопросы
Ответы на контрольные вопросы приведены в файле `questions.md`.

## Выводы

1. **Корректность вычислений:**
   * Все реализованные алгоритмы (редукция и префиксная сумма) на GPU показывают корректные результаты, полностью совпадающие с эталонными вычислениями на CPU с учетом погрешности float.
   * Это подтверждает правильность реализации параллельных алгоритмов, корректное использование shared memory и синхронизацию потоков.

2. **Сравнение производительности CPU и GPU:**
   * Последовательная CPU-реализация работает стабильно, но время выполнения растет линейно с размером массива.
   * GPU с использованием **shared memory** показывает значительное ускорение на больших массивах (10 000 элементов и более), часто в десятки и сотни раз быстрее CPU.
   * На маленьких массивах (10–100 элементов) накладные расходы на запуск кернела и копирование данных из/в GPU могут превышать выигрыш, поэтому CPU иногда работает быстрее.

3. **Влияние типа памяти CUDA:**
   * **Глобальная память:** проста в использовании, но медленная. На больших массивах алгоритмы, полностью завязанные на глобальной памяти, теряют эффективность.
   * **Shared memory:** обеспечивает быстрый доступ внутри блока потоков, позволяет сократить количество обращений к глобальной памяти, существенно ускоряя редукцию и префиксную сумму.
   * Использование shared memory является ключевым моментом оптимизации для параллельных алгоритмов на GPU.

4. **Эффективность масштабирования:**
   * Алгоритмы хорошо масштабируются с ростом объема данных: при увеличении размера массива ускорение GPU относительно CPU растет почти пропорционально.
   * Оптимизация количества потоков и блоков также влияет на производительность и позволяет максимально использовать вычислительные возможности GPU.

5. **Практическое значение и применение:**
   * Редукция (суммирование) и префиксная сумма — базовые операции во многих областях: обработка данных, численные методы, графика, алгоритмы сортировки, параллельные вычисления.
   * Понимание различий между типами памяти и их правильное использование позволяет писать высокопроизводительные параллельные программы на GPU.

6. **Рекомендации по оптимизации:**
   * Для больших массивов всегда использовать shared memory и минимизировать обращения к глобальной памяти.
   * На маленьких массивах параллелизация может быть неэффективной — стоит учитывать накладные расходы.
   * Для сложных алгоритмов (например, Blelloch Scan) важно правильно организовать распределение блоков и учесть смещения между ними.
