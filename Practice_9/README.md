# Практическая работа №9: Распределённая обработка данных с использованием MPI

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена изучению **распределённых вычислений** с использованием **MPI**.

Основная цель работы — освоить продвинутые операции MPI, включая:

* Передачу сообщений (point-to-point),
* Коллективные операции (`MPI_Scatterv`, `MPI_Reduce`, `MPI_Bcast`, `MPI_Allgather`),
* Распределённое решение задач с использованием нескольких процессов.

В рамках работы реализованы три задачи:

1. Распределённое вычисление среднего значения и стандартного отклонения массива.
2. Распределённое решение системы линейных уравнений методом Гаусса.
3. Параллельный поиск кратчайших путей в графе (алгоритм Флойда-Уоршелла).

Для каждой задачи:

* Замерено время выполнения с помощью `MPI_Wtime()`.
* Проверена корректность результатов для небольших данных.
* Протестировано масштабирование при разных количествах процессов.

## Цель работы

* Изучить основы **распределённых вычислений MPI**.
* Освоить коллективные операции MPI (`Scatterv`, `Reduce`, `Bcast`, `Allgather`).
* Решать **статистические, линейные и графовые задачи** параллельно.
* Исследовать производительность и масштабируемость программ на кластере.

## Структура репозитория

```
.
├── task_1.cpp          # Реализация задания 1
├── task_2.cpp          # Реализация задания 2
├── task_3.cpp          # Реализация задания 3
├── questions.md          # Ответы на контрольные вопросы
├── README.md           # Этот файл
```

---

## Теоретическая часть

### MPI (Message Passing Interface)

* **MPI** — стандарт для параллельных вычислений на распределённых системах.
* Позволяет обмениваться данными между процессами на разных узлах или ядрах.
* Основные коллективные операции:

  * `MPI_Scatterv` — распределение частей массива разным процессам;
  * `MPI_Reduce` — сбор локальных результатов в глобальный результат;
  * `MPI_Bcast` — рассылка данных от одного процесса всем остальным;
  * `MPI_Allgather` — сбор данных с каждого процесса и рассылка всем.

### Распределённые алгоритмы

* **Статистика:** каждый процесс обрабатывает часть массива, затем результаты объединяются.
* **Гаусс:** строки матрицы распределяются между процессами, прямой ход выполняется параллельно, обратный ход собирается на процессе 0.
* **Флойд-Уоршелл:** каждая итерация обновляется локально, затем синхронизируется с другими процессами через `MPI_Allgather`.

---

## Практическая часть

### Задание 1: Распределённое вычисление среднего значения и стандартного отклонения

* Массив случайных чисел создаётся на процессе `rank=0`.
* Используется `MPI_Scatterv` для равномерного распределения элементов между процессами.
* Каждый процесс вычисляет локальные суммы и суммы квадратов.
* Глобальные суммы собираются через `MPI_Reduce`.
* Среднее и стандартное отклонение вычисляются на `rank=0`.

**Пример вывода для массива размером 10:**

```
Array size: 10
First 10 elements: 2 5 7 1 3 4 8 9 0 6
Last 10 elements: 2 5 7 1 3 4 8 9 0 6
Mean: 4.5 | StdDev: 2.87
Execution time: 0.000021 seconds
```

* Корректность проверена для всех размеров массивов: 10, 100, 1000, 10000, 100000, 1000000.
* Время выполнения уменьшается при увеличении числа процессов, но после определённого количества процессов выигрыш снижается из-за накладных расходов на коммуникацию.

### Задание 2: Распределённое решение системы линейных уравнений методом Гаусса

**Цель:**
Реализовать параллельное решение системы линейных уравнений ( A x = b ) методом Гаусса с использованием **MPI** и оценить производительность для разных чисел процессов.

**Описание реализации:**

1. **Инициализация MPI**

   * Каждый процесс получает свой `rank` и общее количество процессов `size`.
   * Замер времени выполнения программы начинается сразу после инициализации с помощью `MPI_Wtime()`.

2. **Генерация системы уравнений**

   * Процесс `rank = 0` создаёт матрицу коэффициентов `A` размером `N×N` и вектор правых частей `b`.
   * Элементы матрицы и вектора выбираются случайным образом (от 1 до 10).
   * Для наглядности система выводится на экран:

```
Original system:
    2     6     3     2     2     1 |     1
    9     2     5     1     7     7 |     8
    9    10     6     5     5     7 |     1
    4     5     8     2     7     8 |     4
   10     1     2     5     9     5 |     4
    2     7     1     5     3     1 |     7
-----------------------------------
```

3. **Разделение данных между процессами**

   * Используется `MPI_Scatterv` для распределения строк матрицы `A` и элементов вектора `b` между процессами.
   * Учтён случай, когда число строк не делится нацело на количество процессов.

4. **Прямой ход метода Гаусса (верхнетреугольная матрица)**

   Для каждой ведущей строки `k`:

   * Определяется процесс-владелец строки `k`.
   * Владелец копирует строку в буфер `pivot_row` и рассылает её всем процессам через `MPI_Bcast`.
   * Каждый процесс модифицирует свои строки, которые находятся ниже строки `k`:

     ```
     local_A[i][j] -= factor * pivot_row[j]
     local_b[i]   -= factor * pivot_row[N]
     ```
   * После всех шагов прямого хода каждая локальная часть матрицы треугольная, готовая к обратному ходу.

5. **Сбор результатов**

   * Используется `MPI_Gatherv` для объединения обработанных частей матрицы `A` и вектора `b` на процессе `rank = 0`.

6. **Обратный ход (решение системы)**

   * Выполняется только процессом `rank = 0`.
   * Вектор решений `x` вычисляется сверху вниз:

     ```
     x[i] = (b[i] - sum(A[i][j] * x[j])) / A[i][i]
     ```
   * Решение выводится на экран:

```
Solution vector x:
-1.579 4.098 -7.215 -5.763 4.760 3.219
```

7. **Время выполнения**

   * Процесс `rank = 0` выводит общее время работы программы:

```
Execution time: 0.000524 seconds.
```

**Особенности реализации и производительность:**

* MPI позволяет каждому процессу работать с **своей частью матрицы**, минимизируя синхронизацию.
* Использование `MPI_Bcast` обеспечивает корректность прямого хода, так как все процессы получают актуальную ведущую строку.
* Обратный ход выполняется централизованно, так как решение треугольной системы проще собрать в одном месте.
* Ускорение программы достигается при большом количестве процессов и больших размерах матрицы; для маленьких систем накладные расходы на коммуникацию могут превышать выигрыш.

**Выводы по заданию 2:**

1. **MPI упрощает распределение строк и синхронизацию данных.**
2. При увеличении числа процессов прямой ход метода Гаусса ускоряется, однако для маленьких систем накладные расходы на передачу данных преобладают.
3. Для больших матриц (`N > 100`) параллельный метод значительно сокращает время решения.
4. Метод демонстрирует **корректность** и совпадение решений с последовательной реализацией.


### Задание 3: Параллельный анализ графов (поиск кратчайших путей, алгоритм Флойда-Уоршелла)

**Цель:**
Реализовать параллельный алгоритм поиска кратчайших путей в графе с использованием **MPI**, оценить масштабируемость при разном числе процессов.

**Описание реализации:**

1. **Инициализация MPI**

   * Каждый процесс получает свой `rank` и общее число процессов `size`.
   * Замер времени выполнения начинается сразу после инициализации с помощью `MPI_Wtime()`.

2. **Генерация графа**

   * Процесс `rank = 0` создаёт матрицу смежности `G` размером `N×N`.
   * Элементы матрицы:

     * `0` на диагонали,
     * случайный вес от `1` до `10`,
     * `INF` (отсутствие ребра) с вероятностью 20%.
   * Для графов `N < 11` исходная матрица выводится на экран.

3. **Разделение строк между процессами**

   * Используется `MPI_Scatterv`, чтобы распределить строки матрицы между процессами.
   * Каждый процесс получает локальные строки в массив `local_rows`.

4. **Параллельный алгоритм Флойда-Уоршелла**

   Для каждой вершины `k`:

   * Определяется процесс-владелец строки `k`.
   * Владелец строки рассылает её всем процессам через `MPI_Bcast`.
   * Каждый процесс обновляет свои локальные строки:

     ```
     local_rows[i][j] = min(local_rows[i][j], local_rows[i][k] + k_row[j])
     ```
   * После релаксации используется `MPI_Allgatherv` для синхронизации всех строк между процессами.

5. **Сбор и вывод результатов**

   * Итоговая матрица собирается на процессе `rank = 0` через `MPI_Gatherv`.
   * Для графов `N < 11` выводится полная матрица кратчайших путей.
   * Выводится время выполнения программы для каждого размера графа.

**Пример вывода (4 процесса, маленькие графы):**

```
Original adjacency matrix:
    0  INF    6
    2    0    1
    9    5    0
-----------------------------------
Shortest path matrix:
    0   11    6
    2    0    1
    7    5    0
-----------------------------------
Graph size: 3 | Execution time: 0.000373 seconds.
```

**Результаты производительности при 4 процессах:**

| Размер графа | Время выполнения (сек) | Процессов |
| ------------ | ---------------------- | --------- |
| 3            | 0.000373               | 4         |
| 5            | 0.000611               | 4         |
| 7            | 0.002321               | 4         |
| 10           | 0.002825               | 4         |
| 50           | 0.006621               | 4         |
| 100          | 0.025783               | 4         |
| 150          | 0.082295               | 4         |
| 200          | 0.178229               | 4         |
| 500          | 1.53324                | 4         |
| 1000         | 13.8427                | 4         |

**Примечание:**
Скриншоты в репозитории показывают **результаты при 1 процессе**. Сравнение времени работы при 1 и 4 процессах демонстрирует **масштабируемость и ускорение параллельного алгоритма**.

**Выводы по заданию 3:**

1. **MPI эффективно распределяет нагрузку** между процессами.
2. Для маленьких графов накладные расходы на коммуникацию могут превышать время вычислений.
3. Для больших графов (`N > 100`) параллельная обработка даёт значительное ускорение.
4. Использование `MPI_Allgatherv` обеспечивает корректность и консистентность данных на каждом шаге алгоритма.
5. Увеличение числа процессов сокращает время выполнения до определённого предела; слишком много процессов для маленьких графов может даже увеличить накладные расходы.


## Контрольные вопросы
Ответы на контрольные вопросы приведены в файле questions.md.

## Заключение

Практическая работа помогла:

* Освоить **распределённые операции MPI**.
* Реализовать задачи по статистическому анализу, линейной алгебре и графам.
* Провести **замеры производительности** и исследовать масштабируемость.
* Понять влияние коммуникации и распределения нагрузки на эффективность параллельных программ.

Эта работа является хорошей основой для **дальнейшего изучения параллельного программирования и HPC**.

