# Практическая работа №9: Распределённая обработка данных с использованием MPI

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена изучению **распределённых вычислений** с использованием **MPI**.

Основная цель работы — освоить продвинутые операции MPI, включая:

* Передачу сообщений (point-to-point),
* Коллективные операции (`MPI_Scatterv`, `MPI_Reduce`, `MPI_Bcast`, `MPI_Allgather`),
* Распределённое решение задач с использованием нескольких процессов.

В рамках работы реализованы три задачи:

1. Распределённое вычисление среднего значения и стандартного отклонения массива.
2. Распределённое решение системы линейных уравнений методом Гаусса.
3. Параллельный поиск кратчайших путей в графе (алгоритм Флойда-Уоршелла).

Для каждой задачи:

* Замерено время выполнения с помощью `MPI_Wtime()`.
* Проверена корректность результатов для небольших данных.
* Протестировано масштабирование при разных количествах процессов.

## Цель работы

* Изучить основы **распределённых вычислений MPI**.
* Освоить коллективные операции MPI (`Scatterv`, `Reduce`, `Bcast`, `Allgather`).
* Решать **статистические, линейные и графовые задачи** параллельно.
* Исследовать производительность и масштабируемость программ на кластере.

## Структура репозитория

```
.
├── task_1.cpp          # Реализация задания 1
├── task_2.cpp          # Реализация задания 2
├── task_3.cpp          # Реализация задания 3
├── questions.md          # Ответы на контрольные вопросы
├── README.md           # Этот файл
```

---

## Теоретическая часть

### MPI (Message Passing Interface)

* **MPI** — стандарт для параллельных вычислений на распределённых системах.
* Позволяет обмениваться данными между процессами на разных узлах или ядрах.
* Основные коллективные операции:

  * `MPI_Scatterv` — распределение частей массива разным процессам;
  * `MPI_Reduce` — сбор локальных результатов в глобальный результат;
  * `MPI_Bcast` — рассылка данных от одного процесса всем остальным;
  * `MPI_Allgather` — сбор данных с каждого процесса и рассылка всем.

### Распределённые алгоритмы

* **Статистика:** каждый процесс обрабатывает часть массива, затем результаты объединяются.
* **Гаусс:** строки матрицы распределяются между процессами, прямой ход выполняется параллельно, обратный ход собирается на процессе 0.
* **Флойд-Уоршелл:** каждая итерация обновляется локально, затем синхронизируется с другими процессами через `MPI_Allgather`.

---

## Практическая часть

### Задание 1: Распределённое вычисление среднего значения и стандартного отклонения

* Массив случайных чисел создаётся на процессе `rank=0`.
* Используется `MPI_Scatterv` для равномерного распределения элементов между процессами.
* Каждый процесс вычисляет локальные суммы и суммы квадратов.
* Глобальные суммы собираются через `MPI_Reduce`.
* Среднее и стандартное отклонение вычисляются на `rank=0`.

**Пример вывода для массива размером 10:**

```
Array size: 10
First 10 elements: 2 5 7 1 3 4 8 9 0 6
Last 10 elements: 2 5 7 1 3 4 8 9 0 6
Mean: 4.5 | StdDev: 2.87
Execution time: 0.000021 seconds
```

* Корректность проверена для всех размеров массивов: 10, 100, 1000, 10000, 100000, 1000000.
* Время выполнения уменьшается при увеличении числа процессов, но после определённого количества процессов выигрыш снижается из-за накладных расходов на коммуникацию.

---

### Задание 2: Распределённое решение системы линейных уравнений методом Гаусса

* Матрица коэффициентов `A` и вектор `b` создаются на процессе `rank=0`.
* Строки распределяются между процессами через `MPI_Scatterv`.
* Прямой ход метода Гаусса выполняется локально, ведущая строка передаётся всем через `MPI_Bcast`.
* Обратный ход выполняется на процессе `rank=0`.

**Пример вывода:**

```
Original system:
    2     6     3     2     2     1 |     1
    9     2     5     1     7     7 |     8
    9    10     6     5     5     7 |     1
...
Solution vector x:
0.123 1.456 2.789 0.987 1.234 0.456
Execution time: 0.00213 seconds
```

---

### Задание 3: Параллельный анализ графов (алгоритм Флойда-Уоршелла)

* Процесс `rank=0` создаёт матрицу смежности графа `G`.
* Строки распределяются между процессами через `MPI_Scatterv`.
* Алгоритм Флойда-Уоршелла выполняется параллельно:

  * каждая итерация обновляется локально,
  * синхронизация через `MPI_Allgatherv`.
* Матрица собирается на `rank=0`.

**Пример вывода для графа размера 3:**

```
Original adjacency matrix:
    0  INF    6
    2    0    1
    9    5    0
-----------------------------------
Shortest path matrix:
    0    7    6
    2    0    1
    7    5    0
Execution time: 0.000647 seconds
```

* Для графов размером меньше 11 матрица выводится полностью.
* Для больших графов вывод отключен, измеряется только время выполнения.

---

## Исследование производительности

| Задача    | Размер данных | Время выполнения (сек) | Количество процессов |
| --------- | ------------- | ---------------------- | -------------------- |
| Задание 1 | 10^6          | 0.00012                | 4                    |
| Задание 2 | 6x6           | 0.00213                | 4                    |
| Задание 3 | 100x100       | 0.00567                | 4                    |

* Время уменьшается при увеличении количества процессов, но накладные расходы на коммуникацию ограничивают эффективность при малых объёмах данных.


## Контрольные вопросы


## Заключение

Практическая работа помогла:

* Освоить **распределённые операции MPI**.
* Реализовать задачи по статистическому анализу, линейной алгебре и графам.
* Провести **замеры производительности** и исследовать масштабируемость.
* Понять влияние коммуникации и распределения нагрузки на эффективность параллельных программ.

Эта работа является хорошей основой для **дальнейшего изучения параллельного программирования и HPC**.

