# Практическая работа №9: Распределённая обработка данных с использованием MPI

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена изучению **распределённых вычислений** с использованием **MPI**.

Основная цель работы — освоить продвинутые операции MPI, включая:

* Передачу сообщений (point-to-point),
* Коллективные операции (`MPI_Scatterv`, `MPI_Reduce`, `MPI_Bcast`, `MPI_Allgather`),
* Распределённое решение задач с использованием нескольких процессов.

В рамках работы реализованы три задачи:

1. Распределённое вычисление среднего значения и стандартного отклонения массива.
2. Распределённое решение системы линейных уравнений методом Гаусса.
3. Параллельный поиск кратчайших путей в графе (алгоритм Флойда-Уоршелла).

Для каждой задачи:

* Замерено время выполнения с помощью `MPI_Wtime()`.
* Проверена корректность результатов для небольших данных.
* Протестировано масштабирование при разных количествах процессов.

## Цель работы

* Изучить основы **распределённых вычислений MPI**.
* Освоить коллективные операции MPI (`Scatterv`, `Reduce`, `Bcast`, `Allgather`).
* Решать **статистические, линейные и графовые задачи** параллельно.
* Исследовать производительность и масштабируемость программ на кластере.

## Структура репозитория

```
.
├── task_1.cpp          # Реализация задания 1
├── task_2.cpp          # Реализация задания 2
├── task_3.cpp          # Реализация задания 3
├── questions.md          # Ответы на контрольные вопросы
├── README.md           # Этот файл
```

---

## Теоретическая часть

### MPI (Message Passing Interface)

* **MPI** — стандарт для параллельных вычислений на распределённых системах.
* Позволяет обмениваться данными между процессами на разных узлах или ядрах.
* Основные коллективные операции:

  * `MPI_Scatterv` — распределение частей массива разным процессам;
  * `MPI_Reduce` — сбор локальных результатов в глобальный результат;
  * `MPI_Bcast` — рассылка данных от одного процесса всем остальным;
  * `MPI_Allgather` — сбор данных с каждого процесса и рассылка всем.

### Распределённые алгоритмы

* **Статистика:** каждый процесс обрабатывает часть массива, затем результаты объединяются.
* **Гаусс:** строки матрицы распределяются между процессами, прямой ход выполняется параллельно, обратный ход собирается на процессе 0.
* **Флойд-Уоршелл:** каждая итерация обновляется локально, затем синхронизируется с другими процессами через `MPI_Allgather`.

---

## Практическая часть

### Задание 1: Распределённое вычисление среднего значения и стандартного отклонения

* Массив случайных чисел создаётся на процессе `rank=0`.
* Используется `MPI_Scatterv` для равномерного распределения элементов между процессами.
* Каждый процесс вычисляет локальные суммы и суммы квадратов.
* Глобальные суммы собираются через `MPI_Reduce`.
* Среднее и стандартное отклонение вычисляются на `rank=0`.

**Пример вывода для массива размером 10:**

```
Array size: 10
First 10 elements: 2 5 7 1 3 4 8 9 0 6
Last 10 elements: 2 5 7 1 3 4 8 9 0 6
Mean: 4.5 | StdDev: 2.87
Execution time: 0.000021 seconds
```

* Корректность проверена для всех размеров массивов: 10, 100, 1000, 10000, 100000, 1000000.
* Время выполнения уменьшается при увеличении числа процессов, но после определённого количества процессов выигрыш снижается из-за накладных расходов на коммуникацию.

---

### Задание 2: Распределённое решение системы линейных уравнений методом Гаусса

* Матрица коэффициентов `A` и вектор `b` создаются на процессе `rank=0`.
* Строки распределяются между процессами через `MPI_Scatterv`.
* Прямой ход метода Гаусса выполняется локально, ведущая строка передаётся всем через `MPI_Bcast`.
* Обратный ход выполняется на процессе `rank=0`.

**Пример вывода:**

```
Original system:
    2     6     3     2     2     1 |     1
    9     2     5     1     7     7 |     8
    9    10     6     5     5     7 |     1
...
Solution vector x:
0.123 1.456 2.789 0.987 1.234 0.456
Execution time: 0.00213 seconds
```


### Задание 3: Параллельный анализ графов (поиск кратчайших путей, алгоритм Флойда-Уоршелла)

**Цель:**
Реализовать параллельный алгоритм поиска кратчайших путей в графе с использованием **MPI**, оценить масштабируемость при разном числе процессов.

**Описание реализации:**

1. **Инициализация MPI**

   * Каждый процесс получает свой `rank` и общее число процессов `size`.
   * Замер времени выполнения начинается сразу после инициализации с помощью `MPI_Wtime()`.

2. **Генерация графа**

   * Процесс `rank = 0` создаёт матрицу смежности `G` размером `N×N`.
   * Элементы матрицы:

     * `0` на диагонали,
     * случайный вес от `1` до `10`,
     * `INF` (отсутствие ребра) с вероятностью 20%.
   * Для графов `N < 11` исходная матрица выводится на экран.

3. **Разделение строк между процессами**

   * Используется `MPI_Scatterv`, чтобы распределить строки матрицы между процессами.
   * Каждый процесс получает локальные строки в массив `local_rows`.

4. **Параллельный алгоритм Флойда-Уоршелла**

   Для каждой вершины `k`:

   * Определяется процесс-владелец строки `k`.
   * Владелец строки рассылает её всем процессам через `MPI_Bcast`.
   * Каждый процесс обновляет свои локальные строки:

     ```
     local_rows[i][j] = min(local_rows[i][j], local_rows[i][k] + k_row[j])
     ```
   * После релаксации используется `MPI_Allgatherv` для синхронизации всех строк между процессами.

5. **Сбор и вывод результатов**

   * Итоговая матрица собирается на процессе `rank = 0` через `MPI_Gatherv`.
   * Для графов `N < 11` выводится полная матрица кратчайших путей.
   * Выводится время выполнения программы для каждого размера графа.

**Пример вывода (4 процесса, маленькие графы):**

```
Original adjacency matrix:
    0  INF    6
    2    0    1
    9    5    0
-----------------------------------
Shortest path matrix:
    0   11    6
    2    0    1
    7    5    0
-----------------------------------
Graph size: 3 | Execution time: 0.000373 seconds.
```

**Результаты производительности при 4 процессах:**

| Размер графа | Время выполнения (сек) | Процессов |
| ------------ | ---------------------- | --------- |
| 3            | 0.000373               | 4         |
| 5            | 0.000611               | 4         |
| 7            | 0.002321               | 4         |
| 10           | 0.002825               | 4         |
| 50           | 0.006621               | 4         |
| 100          | 0.025783               | 4         |
| 150          | 0.082295               | 4         |
| 200          | 0.178229               | 4         |
| 500          | 1.53324                | 4         |
| 1000         | 13.8427                | 4         |

**Примечание:**
Скриншоты в репозитории показывают **результаты при 1 процессе**. Сравнение времени работы при 1 и 4 процессах демонстрирует **масштабируемость и ускорение параллельного алгоритма**.

**Выводы по заданию 3:**

1. **MPI эффективно распределяет нагрузку** между процессами.
2. Для маленьких графов накладные расходы на коммуникацию могут превышать время вычислений.
3. Для больших графов (`N > 100`) параллельная обработка даёт значительное ускорение.
4. Использование `MPI_Allgatherv` обеспечивает корректность и консистентность данных на каждом шаге алгоритма.
5. Увеличение числа процессов сокращает время выполнения до определённого предела; слишком много процессов для маленьких графов может даже увеличить накладные расходы.


## Контрольные вопросы
Ответы на контрольные вопросы приведены в файле questions.md.

## Заключение

Практическая работа помогла:

* Освоить **распределённые операции MPI**.
* Реализовать задачи по статистическому анализу, линейной алгебре и графам.
* Провести **замеры производительности** и исследовать масштабируемость.
* Понять влияние коммуникации и распределения нагрузки на эффективность параллельных программ.

Эта работа является хорошей основой для **дальнейшего изучения параллельного программирования и HPC**.

