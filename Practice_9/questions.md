## 1. Как изменяется время выполнения программы при увеличении количества процессов? Почему?

* **Общее правило:**
  Время выполнения программы обычно **уменьшается**, когда количество процессов растёт, потому что работу можно распределить между несколькими процессами. Например, каждая часть массива или строки матрицы обрабатывается одновременно разными процессами.

* **Однако есть нюансы:**

  1. **Накладные расходы MPI:**

     * Передача данных между процессами (`Scatter`, `Allgather`, `Reduce`, `Bcast`) занимает время.
     * Если процессов слишком много, а данные маленькие, **затраты на коммуникацию могут превысить выгоду от параллелизма**.
  2. **Баланс нагрузки:**

     * Если N не делится на количество процессов ровно, часть процессов может работать меньше или больше — возникает **неравномерная нагрузка**, и быстрые процессы ждут медленные.
  3. **Аппаратные ограничения:**

     * На одном компьютере есть **лимит потоков или ядер**. Добавление процессов больше числа ядер не ускоряет работу.

**Итого:**

* Для небольших задач увеличение процессов не всегда даёт выигрыш.
* Для больших задач время уменьшается до момента, пока накладные расходы на обмен данными не начинают доминировать.

## 2. Какие факторы могут влиять на производительность программы?

Основные факторы:

1. **Количество процессов и баланс нагрузки:**

   * Неровное распределение строк/элементов массива замедляет работу.
2. **Скорость обмена данными (MPI коммуникации):**

   * Частые `MPI_Bcast`, `MPI_Reduce` и `MPI_Allgatherv` могут стать узким местом.
3. **Размер задачи:**

   * Малые массивы/матрицы не используют полностью все процессы.
   * Большие массивы увеличивают вычислительную нагрузку, но и коммуникации тоже.
4. **Аппаратные характеристики:**

   * Количество ядер, частота процессора, память, сетевой канал (для кластеров).
5. **Алгоритмические особенности:**

   * Флойд-Уоршелл — алгоритм с O(N³). Даже при параллельной обработке большая часть времени уходит на вычисления.
   * Метод Гаусса — также O(N³).

## 3. Как можно оптимизировать передачу данных между процессами?

В MPI есть несколько стратегий:

1. **Использовать коллективные операции вместо точечных сообщений**

   * Например, `MPI_Allgatherv` или `MPI_Reduce` быстрее, чем несколько `MPI_Send`/`MPI_Recv`.

2. **Сократить количество коммуникаций**

   * Объединять несколько операций в одну.
   * Например, при Флойде-Уоршелле можно передавать сразу несколько строк, если это возможно.

3. **Использовать правильный тип данных и contiguous memory**

   * Передавать **плоские массивы** (`vector<double>`), а не `vector<vector<double>>`, чтобы MPI мог копировать память быстро.

4. **Сократить объём данных**

   * Можно использовать `float` вместо `double`, если точность достаточна.
   * Для больших графов с редкими связями использовать **разреженные структуры**.

5. **Балансировать нагрузку**

   * Раздавать строки равномерно, учитывая остаток при делении (как мы делали через `sendcounts` и `displs`).


## 4. Какие ограничения возникают при работе с большими данными?

1. **Ограничения памяти:**

   * Матрицы NxN занимают O(N²) памяти. Для `N = 10000` это уже ~800 МБ на `double`.
   * Для очень больших графов может не хватить памяти на одном процессе.

2. **Ограничения коммуникаций:**

   * При больших матрицах частые `MPI_Allgather`/`MPI_Reduce` занимают значительное время.
   * Узким местом становится сеть (для кластера) или системная шина (для одного компьютера).

3. **Сложность алгоритмов:**

   * Флойд-Уоршелл и Гаусс — O(N³). Даже на 1000 строках время выполнения растёт очень быстро.

4. **Числовые ошибки:**

   * Для очень больших матриц или чисел может появляться **погрешность при сложении больших чисел** или делении.
