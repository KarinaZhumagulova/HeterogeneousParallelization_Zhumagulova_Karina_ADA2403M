# Практическая работа №4: Оптимизация параллельного кода на GPU с использованием различных типов памяти

**Astana IT University**  

**Курс:** Heterogeneous Parallelization  

**Преподаватель:** Садвакасова Куралай Жанжигитовна  

**Студент:** Жумагулова Карина  

**Группа:** ADA-2403M  

**Дата:** 17.01.2026  

## Краткое описание практической работы

Данная практическая работа посвящена исследованию и оптимизации параллельных вычислений на GPU с использованием различных типов памяти CUDA. В рамках работы реализованы и проанализированы алгоритмы редукции и сортировки с применением глобальной, разделяемой и локальной памяти, а также выполнено сравнение их производительности.

## Цель работы

Изучить особенности использования различных типов памяти CUDA (глобальной, разделяемой и локальной) и оценить их влияние на производительность параллельных алгоритмов при выполнении вычислений на графическом процессоре.

## Структура репозитория

```
.
├── task_1.cu        # Параллельная сортировка слиянием на GPU
├── task_2.cu        # Параллельная быстрая сортировка на GPU
├── task_3.cu        # Параллельная пирамидальная сортировка на GPU
├── task_4.cpp       # Последовательные версии сортировок на CPU
├── practice_work_4.cu  # Реализация редукции и сортировки с использованием разных типов памяти CUDA
├── practice_work_4.ipynb # Jupyter Notebook для запуска и тестирования программы
├── questions.md     # Контрольные вопросы и ответы по работе
├── README.md        # Описание проекта
```

## Теоретическая часть
**Типы памяти в CUDA**

 - **Глобальная память**
   Доступна всем потокам и всем блокам, но имеет относительно высокую задержку доступа. Используется для хранения больших массивов данных.

- **Разделяемая (Shared) память**
  Быстрая память, общая для потоков одного блока. Эффективна для обмена данными и промежуточных вычислений внутри блока.

- **Локальная память**
  Доступна только одному потоку, обычно хранится в регистрах. Обладает высокой скоростью доступа, но сильно ограничена по объему.

**Оптимизация с использованием памяти**

- Минимизация количества обращений к глобальной памяти за счет использования разделяемой памяти.
- Повышение пропускной способности за счет коалесцированного доступа к глобальной памяти.
- Использование локальной памяти для хранения временных переменных и промежуточных результатов.

В качестве примера рассматривается задача параллельного суммирования элементов массива, где глобальная память используется для хранения исходных данных, а разделяемая — для промежуточных сумм внутри блока.

## Практическая часть
**Общая характеристика**
Все задания практической работы реализованы в одном CUDA-файле practice_work_4.cu. Программа генерирует массивы случайных чисел различных размеров, выполняет редукцию и сортировку с использованием разных типов памяти CUDA, а также измеряет время выполнения операций.

## Задание 1. Подготовка данных

### Описание задания

Реализовать генерацию массива случайных чисел заданного размера (до 1 000 000 элементов).

### Цель задания

Подготовить входные данные для последующих параллельных вычислений.

### Файлы проекта

* `practice_work_4.cu`

### Функционал программы

* Генерация массива случайных целых чисел на CPU.
* Копирование данных в память GPU.

### Используемые технологии

* C++
* CUDA Runtime API

### Особенности реализации

Используется генератор случайных чисел `mt19937` и равномерное распределение.

## Задание 2. Оптимизация параллельного редукционного алгоритма

### Описание задания

Реализовать два варианта редукции суммы элементов массива:

* с использованием только глобальной памяти;
* с использованием комбинации глобальной и разделяемой памяти.

### Цель задания

Сравнить производительность редукции при использовании разных типов памяти CUDA.

### Файлы проекта

* `practice_work_4.cu`

### Функционал программы

* Параллельное суммирование элементов массива.
* Использование атомарных операций.
* Замер времени выполнения с помощью CUDA Events.

### Используемые технологии

* CUDA kernels
* Atomic operations
* Shared memory

### Особенности реализации

Редукция с разделяемой памятью использует древовидный алгоритм суммирования внутри блока, что позволяет существенно снизить количество обращений к глобальной памяти.

## Задание 3. Оптимизация сортировки на GPU

### Описание задания

Реализовать сортировку пузырьком для подмассивов внутри блоков GPU с последующим слиянием результатов.

### Цель задания

Исследовать влияние локальной и разделяемой памяти на производительность сортировки.

### Файлы проекта

* `practice_work_4.cu`

### Функционал программы

* Локальная сортировка элементов внутри блока GPU.
* Использование разделяемой памяти для ускорения доступа.
* Финальная досортировка массива на CPU.

### Используемые технологии

* CUDA kernels
* Shared memory
* STL `std::sort` (финальный этап)

### Особенности реализации

GPU сортирует данные только внутри независимых блоков, поэтому для получения полностью отсортированного массива используется дополнительная сортировка на CPU.

## Задание 4. Измерение производительности

### Описание задания

Измерить время выполнения редукции и сортировки для массивов размером:

* 10 000
* 100 000
* 1 000 000 элементов

### Цель задания

Проанализировать зависимость времени выполнения алгоритмов от размера массива и типа используемой памяти.

### Функционал программы

* Замер времени выполнения операций с использованием CUDA Events.
* Сравнение глобальной и разделяемой памяти.
* Подготовка данных для построения графиков производительности.

## Выводы

В ходе выполнения практической работы были изучены различные типы памяти CUDA и их влияние на производительность параллельных алгоритмов. Эксперименты показали, что использование разделяемой памяти позволяет значительно сократить время выполнения редукции по сравнению с использованием только глобальной памяти. Реализация сортировки на GPU демонстрирует корректность работы, однако для больших массивов требует более оптимальных алгоритмов.

## Контрольные вопросы

Ответы на контрольные вопросы представлены в файле `questions.md`.

