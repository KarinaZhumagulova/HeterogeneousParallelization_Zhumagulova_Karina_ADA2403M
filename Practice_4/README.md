# Практическая работа №4: Оптимизация параллельного кода на GPU с использованием различных типов памяти

**Astana IT University**  

**Курс:** Heterogeneous Parallelization  

**Преподаватель:** Садвакасова Куралай Жанжигитовна  

**Студент:** Жумагулова Карина  

**Группа:** ADA-2403M  

**Дата:** 17.01.2026  

## Краткое описание практической работы

Данная практическая работа посвящена исследованию и оптимизации параллельных вычислений на GPU с использованием различных типов памяти CUDA. В рамках работы реализованы и проанализированы алгоритмы редукции и сортировки с применением глобальной, разделяемой и локальной памяти, а также выполнено сравнение их производительности.

## Цель работы

Изучить особенности использования различных типов памяти CUDA (глобальной, разделяемой и локальной) и оценить их влияние на производительность параллельных алгоритмов при выполнении вычислений на графическом процессоре.

## Структура репозитория

```
.
├── task_1.cu        # Параллельная сортировка слиянием на GPU
├── task_2.cu        # Параллельная быстрая сортировка на GPU
├── task_3.cu        # Параллельная пирамидальная сортировка на GPU
├── task_4.cpp       # Последовательные версии сортировок на CPU
├── practice_work_4.cu  # Реализация редукции и сортировки с использованием разных типов памяти CUDA
├── practice_work_4.ipynb # Jupyter Notebook для запуска и тестирования программы
├── questions.md     # Контрольные вопросы и ответы по работе
├── README.md        # Описание проекта
```

## Теоретическая часть
**Типы памяти в CUDA**

 - **Глобальная память**
   Доступна всем потокам и всем блокам, но имеет относительно высокую задержку доступа. Используется для хранения больших массивов данных.

- **Разделяемая (Shared) память**
  Быстрая память, общая для потоков одного блока. Эффективна для обмена данными и промежуточных вычислений внутри блока.

- **Локальная память**
  Доступна только одному потоку, обычно хранится в регистрах. Обладает высокой скоростью доступа, но сильно ограничена по объему.

**Оптимизация с использованием памяти**

- Минимизация количества обращений к глобальной памяти за счет использования разделяемой памяти.
- Повышение пропускной способности за счет коалесцированного доступа к глобальной памяти.
- Использование локальной памяти для хранения временных переменных и промежуточных результатов.

В качестве примера рассматривается задача параллельного суммирования элементов массива, где глобальная память используется для хранения исходных данных, а разделяемая — для промежуточных сумм внутри блока.

## Практическая часть
**Общая характеристика**
Все задания практической работы реализованы в одном CUDA-файле practice_work_4.cu. Программа генерирует массивы случайных чисел различных размеров, выполняет редукцию и сортировку с использованием разных типов памяти CUDA, а также измеряет время выполнения операций.

## Задание 1. Подготовка данных

### Описание задания

Реализовать генерацию массива случайных чисел заданного размера (до 1 000 000 элементов).

### Цель задания

Подготовить входные данные для последующих параллельных вычислений.

### Файлы проекта

* `practice_work_4.cu`
* `HP_Practice_4.ipynb`

### Функционал программы

* Генерация массива случайных целых чисел на CPU.
* Копирование данных в память GPU.

### Используемые технологии

* C++
* CUDA Runtime API
* Google Colab / Jupiter Noteboook

### Особенности реализации

Используется генератор случайных чисел `mt19937` и равномерное распределение.

## Задание 2. Оптимизация параллельного редукционного алгоритма

### Описание задания

Реализовать два варианта редукции суммы элементов массива:

* с использованием только глобальной памяти;
* с использованием комбинации глобальной и разделяемой памяти.

### Цель задания

Сравнить производительность редукции при использовании разных типов памяти CUDA.

### Файлы проекта

* `practice_work_4.cu`
* `HP_Practice_4.ipynb`

### Функционал программы

* Параллельное суммирование элементов массива.
* Использование атомарных операций.
* Замер времени выполнения с помощью CUDA Events.

### Используемые технологии

* CUDA kernels
* Atomic operations
* Shared memory

### Особенности реализации

Редукция с разделяемой памятью использует древовидный алгоритм суммирования внутри блока, что позволяет существенно снизить количество обращений к глобальной памяти.

## Задание 3. Оптимизация сортировки на GPU

### Описание задания

Реализовать сортировку пузырьком для подмассивов внутри блоков GPU с последующим слиянием результатов.

### Цель задания

Исследовать влияние локальной и разделяемой памяти на производительность сортировки.

### Файлы проекта

* `practice_work_4.cu`
* `HP_Practice_4.ipynb`

### Функционал программы

* Локальная сортировка элементов внутри блока GPU.
* Использование разделяемой памяти для ускорения доступа.
* Финальная досортировка массива на CPU.

### Используемые технологии

* CUDA kernels
* Shared memory
* STL `std::sort` (финальный этап)

### Особенности реализации

GPU сортирует данные только внутри независимых блоков, поэтому для получения полностью отсортированного массива используется дополнительная сортировка на CPU.

## Задание 4. Измерение производительности

### Описание задания

Измерить время выполнения редукции и сортировки для массивов размером:

* 10 000
* 100 000
* 1 000 000 элементов

### Цель задания

Проанализировать зависимость времени выполнения алгоритмов от размера массива и типа используемой памяти.

### Функционал программы

* Замер времени выполнения операций с использованием CUDA Events.
* Сравнение глобальной и разделяемой памяти.
* Подготовка данных для построения графиков производительности.

## Анализ результатов и сравнение производительности

В ходе выполнения практической работы были проведены экспериментальные измерения времени выполнения операций редукции и сортировки для массивов различного размера: от **N = 10** до **N = 1 000 000** элементов. Для редукции использовались два подхода: с обращением к **глобальной памяти GPU** и с использованием **разделяемой памяти (shared memory)**. Также измерялось общее время сортировки, включающее выполнение на GPU и финальное слияние.

<img width="578" height="459" alt="image" src="https://github.com/user-attachments/assets/884dc169-a459-4ca5-a186-9c44ec7a29c0" />

### Сравнение редукции в глобальной и разделяемой памяти

Полученные результаты показывают, что использование разделяемой памяти значительно снижает время выполнения операции редукции по сравнению с глобальной памятью:

* Для **малых массивов (N = 10)** разница наиболее заметна:
  редукция в глобальной памяти занимает **8.55 мс**, тогда как в разделяемой — всего **0.0034 мс**.
  Это объясняется тем, что накладные расходы доступа к глобальной памяти и инициализации потоков доминируют при малых объёмах данных.
* Начиная с **N = 100**, время редукции в глобальной памяти резко уменьшается и становится сопоставимым с временем в разделяемой памяти.
* Для **больших массивов (N ≥ 10 000)** время редукции стабилизируется и практически не зависит от размера входных данных, оставаясь в диапазоне **0.002–0.003 мс** для обоих вариантов.

Использование разделяемой памяти позволяет:

* сократить количество обращений к глобальной памяти,
* уменьшить задержки доступа,
* повысить эффективность параллельных операций внутри одного блока потоков.

### Зависимость времени сортировки от размера массива

Время сортировки демонстрирует ярко выраженную зависимость от размера массива:

* Для **малых массивов (N = 10, 100)** сортировка выполняется за доли миллисекунды (0.02–0.04 мс).
* При увеличении размера до **N = 1 000** время возрастает до **0.29 мс**, а при **N = 10 000** — до **3.16 мс**.
* Для **крупных массивов** наблюдается резкий рост:

  * **N = 100 000** → 35.7 мс
  * **N = 1 000 000** → 436.6 мс

<img width="578" height="459" alt="image" src="https://github.com/user-attachments/assets/9be4e942-5db0-42b9-ac6e-a89ad697346e" />

Такое поведение объясняется:

* увеличением количества операций сравнения и обмена элементов,
* необходимостью финального слияния частично отсортированных данных,
* ростом нагрузки на память и шину передачи данных между GPU и CPU.

Несмотря на рост абсолютного времени, алгоритм демонстрирует корректную масштабируемость и успешно обрабатывает массивы большого размера.

### Выводы по анализу

1. Использование **разделяемой памяти GPU** существенно повышает производительность операции редукции, особенно для малых и средних размеров массивов.
2. Для больших массивов время редукции стабилизируется, что свидетельствует о хорошей оптимизации параллельного алгоритма.
3. Время сортировки напрямую зависит от размера входного массива и возрастает нелинейно, что соответствует теоретической сложности алгоритмов сортировки.
4. GPU-реализация позволяет эффективно обрабатывать большие объёмы данных, однако при увеличении N влияние операций памяти и финального слияния становится доминирующим.
5. Экспериментальные результаты подтверждают целесообразность применения параллельных алгоритмов и иерархии памяти CUDA при работе с большими массивами данных.


## Выводы

В ходе выполнения практической работы были изучены различные типы памяти CUDA и их влияние на производительность параллельных алгоритмов. Эксперименты показали, что использование разделяемой памяти позволяет значительно сократить время выполнения редукции по сравнению с использованием только глобальной памяти. Реализация сортировки на GPU демонстрирует корректность работы, однако для больших массивов требует более оптимальных алгоритмов.

## Контрольные вопросы

Ответы на контрольные вопросы представлены в файле `questions.md`.

