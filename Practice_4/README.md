# Практическая работа №4: Оптимизация параллельного кода на GPU с использованием различных типов памяти

**Astana IT University**  

**Курс:** Heterogeneous Parallelization  

**Преподаватель:** Садвакасова Куралай Жанжигитовна  

**Студент:** Жумагулова Карина  

**Группа:** ADA-2403M  

**Дата:** 17.01.2026  

## Краткое описание практической работы

Данная практическая работа посвящена исследованию и оптимизации параллельных вычислений на GPU с использованием различных типов памяти CUDA. В рамках работы реализованы и проанализированы алгоритмы редукции и сортировки с применением глобальной, разделяемой и локальной памяти, а также выполнено сравнение их производительности.

## Цель работы

Изучить особенности использования различных типов памяти CUDA (глобальной, разделяемой и локальной) и оценить их влияние на производительность параллельных алгоритмов при выполнении вычислений на графическом процессоре.

## Структура репозитория

```
.
├── task_1.cu        # Параллельная сортировка слиянием на GPU
├── task_2.cu        # Параллельная быстрая сортировка на GPU
├── task_3.cu        # Параллельная пирамидальная сортировка на GPU
├── task_4.cpp       # Последовательные версии сортировок на CPU
├── practice_work_4.cu  # Реализация редукции и сортировки с использованием разных типов памяти CUDA
├── practice_work_4.ipynb # Jupyter Notebook для запуска и тестирования программы
├── questions.md     # Контрольные вопросы и ответы по работе
├── README.md        # Описание проекта
```

## Теоретическая часть
**Типы памяти в CUDA**

 - **Глобальная память**
   Доступна всем потокам и всем блокам, но имеет относительно высокую задержку доступа. Используется для хранения больших массивов данных.

- **Разделяемая (Shared) память**
  Быстрая память, общая для потоков одного блока. Эффективна для обмена данными и промежуточных вычислений внутри блока.

- **Локальная память**
- Доступна только одному потоку, обычно хранится в регистрах. Обладает высокой скоростью доступа, но сильно ограничена по объему.

**Оптимизация с использованием памяти**

- Минимизация количества обращений к глобальной памяти за счет использования разделяемой памяти.
- Повышение пропускной способности за счет коалесцированного доступа к глобальной памяти.
- Использование локальной памяти для хранения временных переменных и промежуточных результатов.

В качестве примера рассматривается задача параллельного суммирования элементов массива, где глобальная память используется для хранения исходных данных, а разделяемая — для промежуточных сумм внутри блока.
