# Контрольные вопросы к Практической работе №10

## **1. В чём отличие измерения времени выполнения от профилирования?**

* **Измерение времени выполнения (timing)** – это простой способ узнать, сколько времени занимает выполнение программы или её конкретного участка.

  * Используются функции вроде `omp_get_wtime()` (CPU), `cudaEventElapsedTime()` (GPU), `MPI_Wtime()` (MPI).
  * Пример: засечь время заполнения массива, вычисления суммы или дисперсии.
  * **Цель** – понять общую производительность и оценить, сколько занимает конкретный блок кода.

* **Профилирование (profiling)** – это более детальный анализ работы программы.

  * Инструменты: Intel VTune, NVIDIA Nsight, gprof, perf, nvprof.
  * Позволяет определить:

    * какие функции или ядра занимают больше всего времени;
    * сколько времени тратится на память, вычисления, коммуникацию;
    * узкие места (bottlenecks) и неэффективное использование ресурсов.
  * **Цель** – оптимизация и понимание, что ограничивает производительность.

**Итог:** измерение времени – это «сколько времени», профилирование – «почему занимает столько времени».

## **2. Какие виды узких мест характерны для CPU, GPU и распределённых программ?**

* **CPU**:

  * Узкие места часто связаны с:

    * последовательной частью кода (код, который нельзя распараллелить);
    * неэффективным кэшированием и доступом к памяти;
    * синхронизацией потоков (mutex, barriers, lock).
  * Пример: многопоточный цикл с частыми блокировками сильно замедляет программу.

* **GPU**:

  * Основные узкие места:

    * передача данных между CPU и GPU (Host ↔ Device) – `cudaMemcpy`;
    * малое количество потоков на блок или неэффективное использование shared memory;
    * divergence в ветвлении (разные потоки выполняют разные инструкции);
    * недостаток параллелизма для малых массивов.

* **Распределённые программы (MPI)**:

  * Узкие места:

    * коммуникации между процессами (`MPI_Reduce`, `MPI_Allreduce`, `MPI_Bcast`);
    * нагрузка распределена неравномерно (load imbalance);
    * высокие задержки сети при больших объёмах данных.

**Вывод:** узкие места зависят от архитектуры и типа параллельности: CPU – вычисления и память, GPU – передача данных и divergence, MPI – коммуникации и баланс нагрузки.

## **3. Почему увеличение числа потоков или процессов не всегда приводит к ускорению?**

* **Законы Амдала и Густафсона** объясняют это. Основные причины:

  1. **Последовательная часть программы**: даже если 99% кода распараллелено, 1% последовательного кода ограничивает ускорение.

     * Пример: закон Амдала: максимальное ускорение = `1 / (S + P/N)`, где S – последовательная доля, P – параллельная, N – число потоков.
  2. **Накладные расходы на синхронизацию**:

     * Барьеры, блокировки, атомарные операции увеличивают время с ростом числа потоков.
  3. **Оверхед распределённых систем**:

     * С ростом числа MPI-процессов увеличивается коммуникация (сбор данных, broadcast).
  4. **Ресурсные ограничения**:

     * Ограничения памяти, кэш, шины PCIe (для GPU), сеть.

**Итог:** увеличение числа потоков или процессов улучшает производительность только до определённого предела; дальше накладные расходы и последовательные участки становятся доминирующими.

## **4. Как законы Амдала и Густафсона применяются при анализе масштабируемости?**

* **Закон Амдала (Amdahl’s Law):**

  * Предсказывает ускорение при фиксированном объёме работы (strong scaling).
  * Формула:
    [
    S(N) = \frac{1}{S + \frac{P}{N}}
    ]
    где S – доля последовательного кода, P – параллельная доля, N – число потоков/процессов.
  * **Применение:** помогает понять предел ускорения для фиксированного массива. Даже идеальное параллельное ядро не даст бесконечного ускорения.

* **Закон Густафсона (Gustafson’s Law):**

  * Предсказывает ускорение при увеличении объёма работы пропорционально числу процессов (weak scaling).
  * Формула:
    [
    S(N) = S + P \cdot N
    ]
  * **Применение:** полезен при распределённых вычислениях и гибридных приложениях, когда мы можем увеличивать размер данных с ростом числа процессов.

**Вывод:** Аmdahl – для ограничения ускорения при фиксированной задаче, Gustafson – для оценки масштабируемости при росте объёма задачи.

## **5. Какие факторы наиболее критичны для производительности гибридных приложений?**

Гибридные приложения (CPU + GPU) зависят от множества факторов:

1. **Накладные расходы на передачу данных (H2D и D2H)**:

   * Асинхронные копирования (`cudaMemcpyAsync`) и CUDA streams помогают уменьшить влияние.
2. **Баланс нагрузки между CPU и GPU**:

   * Если CPU простаивает или GPU перегружен – низкая эффективность.
3. **Эффективность GPU-ядра (kernel)**:

   * Grid-stride loops, оптимальное количество потоков на блок, использование shared memory.
4. **Синхронизация и ожидание**:

   * CPU не должен ждать GPU дольше, чем нужно; GPU не должен простаивать из-за CPU.
5. **Память**:

   * Использование pinned memory ускоряет копирование, уменьшает page faults.
6. **Коммуникации в распределённых гибридных системах**:

   * MPI + GPU: нужно минимизировать глобальные операции `MPI_Reduce`, избегать частых барьеров.

**Вывод:** производительность гибридного приложения определяется **сбалансированностью вычислений и коммуникаций**, эффективной организацией памяти и минимизацией простоев.
