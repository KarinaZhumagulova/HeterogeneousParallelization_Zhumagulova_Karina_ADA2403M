# Практическая работа №10: Профилирование и оптимизация параллельных и гибридных приложений

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена **анализу производительности, профилированию и оптимизации параллельных программ**, реализованных с использованием технологий:

* **OpenMP** (CPU-параллелизм),
* **CUDA** (GPU-вычисления),
* **MPI** (распределённые вычисления),
* **гибридных CPU + GPU решений**.

Основная цель работы — изучить, **как архитектура вычислений, доступ к памяти и коммуникации между CPU и GPU влияют на производительность**, а также научиться выявлять узкие места и применять оптимизации.

В рамках работы реализованы четыре задания:

1. Анализ производительности CPU-параллельной программы (OpenMP).
2. Анализ и оптимизация доступа к памяти на GPU (CUDA).
3. Профилирование гибридного приложения CPU + GPU.
4. Анализ масштабируемости распределённой программы (MPI).

Для всех заданий:

* выполнены **замеры времени выполнения**,
* проведено **профилирование отдельных этапов вычислений**,
* сделаны выводы о масштабируемости и узких местах.

## Цель работы

* Изучить методы **профилирования параллельных программ**.
* Проанализировать влияние числа потоков и процессов на ускорение.
* Исследовать **накладные расходы передачи данных** между CPU и GPU.
* Освоить оптимизации памяти (coalesced access, shared memory, pinned memory).
* Проанализировать масштабируемость с точки зрения **законов Амдала и Густафсона**.

## Структура репозитория

```
.
├── task_1_openmp.cpp        # OpenMP: анализ CPU-параллелизма
├── task_2_cuda_memory.cu    # CUDA: доступ к памяти и оптимизация
├── task_3_hybrid.cu         # Гибридное CPU + GPU приложение
├── task_4_mpi.cpp           # MPI: strong и weak scaling
├── questions.md             # Ответы на контрольные вопросы
├── README.md                # Этот файл
```

## Теоретическая часть

### Профилирование параллельных программ

**Профилирование** — это процесс измерения:

* времени выполнения отдельных частей программы,
* загрузки вычислительных ресурсов,
* накладных расходов синхронизации и передачи данных.

В данной работе используются:

* `omp_get_wtime()` — для CPU (OpenMP),
* `cudaEventRecord()` — для GPU,
* `MPI_Wtime()` — для распределённых программ.

## Практическая часть

## Задание 1: Анализ производительности CPU-параллельной программы (OpenMP)

**Цель:**
Реализовать параллельную обработку массива данных с использованием OpenMP и проанализировать влияние числа потоков на ускорение.

**Описание реализации:**

* Вычисление суммы, среднего значения и дисперсии большого массива.
* Использование директив `#pragma omp parallel for`.
* Замер времени выполнения с помощью `omp_get_wtime()`.

### 1. Доля параллельной и последовательной частей программы

**Что видно из данных:**

* Последовательная часть (`Sequential time`) — это **генерация массива случайных чисел** и вывод краёв массива.
* Параллельная часть (`Parallel time`) — это **вычисление суммы, среднего и дисперсии через OpenMP**.

**Наблюдения:**

| Размер массива                   | Доля последовательной части | Доля параллельной части |
| -------------------------------- | --------------------------- | ----------------------- |
| Малые (10–1000)                  | 0.4–0.9                     | 0.1–0.6                 |
| Средние (10 000–1 000 000)       | 0.85–0.95                   | 0.05–0.15               |
| Большие (10 000 000–100 000 000) | 0.90–0.97                   | 0.03–0.10               |

**Выводы:**

* Для **малых массивов** параллельная часть занимает значительную долю (иногда до 60%).
* Для **больших массивов** почти 90–97% времени уходит на **последовательную генерацию данных**, и параллельные вычисления занимают только 3–10%.
* Это объясняет, почему ускорение с ростом потоков не такое большое — последовательная часть сильно ограничивает общую производительность.

### 2. Влияние числа потоков на ускорение

Ускорение можно оценить по формуле:

[
S = \frac{T_1}{T_p}
]

где (T_1) — общее время на 1 потоке, (T_p) — общее время на p потоках.

**Примеры для больших массивов (100 000 000):**

| Потоки | Total time (s) | Ускорение (S = T_1 / T_p) |
| ------ | -------------- | ------------------------- |
| 1      | 11.744         | 1.00                      |
| 2      | 10.683         | 1.10                      |
| 4      | 10.957         | 1.07                      |
| 8      | 10.129         | 1.16                      |

**Выводы:**

* Для **больших массивов** ускорение крайне слабое (~1.1–1.16x), хотя потоков 8.
* Для **малых массивов** ускорение иногда даже хуже, из-за накладных расходов на создание потоков и синхронизацию.

### 3.  Закон Амдала

**Формула закона Амдала:**

[
S_\text{max} = \frac{1}{(1-P) + \frac{P}{N}}
]

где:

* (P) — доля параллельной части,
* (N) — число потоков,
* (1-P) — последовательная часть.

**Пример для массива 100 000 000, 8 потоков:**

* Параллельная доля (P \approx 0.065) (среднее из таблицы)
* Последовательная часть (1-P \approx 0.935)

[
S_\text{max} = \frac{1}{0.935 + 0.065/8} \approx \frac{1}{0.9431} \approx 1.06
]

**Сравнение с фактическим ускорением:** ~1.16 — близко к расчету (малые расхождения из-за измерений и генерации случайных чисел).

**Выводы по закону Амдала:**

* Ускорение ограничено высокой последовательной частью.
* Даже при увеличении потоков выше 8–16 реального ускорения почти не будет.
* Для улучшения ускорения нужно уменьшить последовательную часть — например, **сделать генерацию случайных чисел тоже параллельной**.

### 4. Итоговые конкретные выводы

1. **Доля последовательной/параллельной частей:**

   * Малые массивы: параллельная часть ~50%
   * Большие массивы: параллельная часть 3–10%, последовательная 90–97%

2. **Влияние числа потоков на ускорение:**

   * С ростом потоков ускорение увеличивается очень слабо для больших массивов.
   * Для малых массивов накладные расходы на OpenMP уменьшают эффективность.

3. **Закон Амдала:**

   * Ограничивает максимальное ускорение из-за высокой доли последовательной части.
   * Для массивов 100 млн элементов, с 8 потоками, теоретическое ускорение ~1.06x.

4. **Рекомендации для оптимизации:**

   * Параллелить генерацию случайных чисел (`#pragma omp parallel for`)
   * Параллельно выводить данные или избегать печати для больших массивов
   * Сосредоточиться на больших вычислительных блоках для OpenMP

### Платформа

* Язык: C++
* Параллельная модель: OpenMP
* Замер времени: `omp_get_wtime()`
* Диапазон значений: `[-100, 100]`
* Количество потоков: `1, 2, 4, 8`

### Результаты экспериментов

#### OpenMP threads = 1

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 1.19e-06            | 1.58e-04          | 1.59e-04       | 0.0075              | 0.9925            |
| 100            | 9.06e-06            | 1.50e-05          | 2.41e-05       | 0.3762              | 0.6238            |
| 1 000          | 1.22e-04            | 1.72e-05          | 1.39e-04       | 0.8767              | 0.1233            |
| 10 000         | 1.32e-03            | 1.41e-04          | 1.46e-03       | 0.9032              | 0.0968            |
| 100 000        | 1.12e-02            | 2.05e-03          | 1.33e-02       | 0.8454              | 0.1546            |
| 1 000 000      | 1.04e-01            | 1.30e-02          | 1.17e-01       | 0.8885              | 0.1115            |
| 10 000 000     | 1.18                | 0.130             | 1.31           | 0.9002              | 0.0998            |
| 100 000 000    | 9.99                | 1.75              | 11.74          | 0.8510              | 0.1490            |

#### OpenMP threads = 2

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 9.54e-07            | 7.97e-04          | 7.98e-04       | 0.0012              | 0.9988            |
| 100            | 7.15e-06            | 2.60e-05          | 3.31e-05       | 0.2158              | 0.7842            |
| 1 000          | 8.51e-05            | 8.70e-05          | 1.72e-04       | 0.4945              | 0.5055            |
| 10 000         | 1.00e-03            | 2.28e-04          | 1.23e-03       | 0.8147              | 0.1853            |
| 100 000        | 1.04e-02            | 9.88e-04          | 1.14e-02       | 0.9136              | 0.0864            |
| 1 000 000      | 9.44e-02            | 7.15e-03          | 0.102          | 0.9296              | 0.0704            |
| 10 000 000     | 0.945               | 0.068             | 1.01           | 0.9327              | 0.0673            |
| 100 000 000    | 9.47                | 1.22              | 10.68          | 0.8862              | 0.1138            |

#### OpenMP threads = 4

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 9.54e-07            | 3.44e-04          | 3.45e-04       | 0.0028              | 0.9972            |
| 100            | 3.79e-05            | 1.17e-04          | 1.55e-04       | 0.2446              | 0.7554            |
| 1 000          | 8.61e-05            | 1.26e-04          | 2.12e-04       | 0.4056              | 0.5944            |
| 10 000         | 1.07e-03            | 1.78e-04          | 1.24e-03       | 0.8570              | 0.1430            |
| 100 000        | 9.16e-03            | 7.55e-04          | 9.91e-03       | 0.9238              | 0.0762            |
| 1 000 000      | 9.12e-02            | 4.40e-03          | 0.0956         | 0.9540              | 0.0460            |
| 10 000 000     | 0.931               | 0.043             | 0.974          | 0.9559              | 0.0441            |
| 100 000 000    | 10.07               | 0.886             | 10.96          | 0.9192              | 0.0808            |

#### OpenMP threads = 8

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 1.91e-06            | 7.36e-04          | 7.38e-04       | 0.0026              | 0.9974            |
| 100            | 1.10e-05            | 5.61e-04          | 5.72e-04       | 0.0192              | 0.9808            |
| 1 000          | 2.77e-04            | 4.18e-04          | 6.95e-04       | 0.3986              | 0.6014            |
| 10 000         | 2.78e-03            | 4.58e-04          | 3.24e-03       | 0.8585              | 0.1415            |
| 100 000        | 1.35e-02            | 1.13e-03          | 1.47e-02       | 0.9227              | 0.0773            |
| 1 000 000      | 0.133               | 3.69e-03          | 0.137          | 0.9730              | 0.0270            |
| 10 000 000     | 1.40                | 0.034             | 1.43           | 0.9762              | 0.0238            |
| 100 000 000    | 9.47                | 0.658             | 10.13          | 0.9351              | 0.0649            |

Скриншот результатов (частично)
<img width="940" height="738" alt="image" src="https://github.com/user-attachments/assets/69163ee0-6cbe-4bf3-b123-852a8583a351" />

## Задание 2: Оптимизация доступа к памяти на GPU (CUDA)

**Цель:**
Реализовать ядро CUDA для обработки массива данных, продемонстрировать влияние разных паттернов доступа к памяти на производительность и оптимизировать доступ с использованием разделяемой памяти.

**Описание реализации:**

1. **Генерация данных на CPU**

   * Создаётся массив `h_data` размером `N` элементов типа `float`.
   * Диапазон случайных чисел: от -100 до 100.
   * Для контроля данных выводятся **первые 10 элементов массива**:

```
First 10 elements: 23 -45 12 0 89 -3 67 12 5 -98
```
2. **Выделение памяти на GPU**

   * Выделяется глобальная память для исходного массива `d_data` и массива результатов `d_output`.
   * Данные копируются с CPU на GPU с помощью `cudaMemcpy`.

3. **Реализация ядер CUDA**

   **a. Некоалесцированный доступ (Non-Coalesced)**

   * Используется **grid-stride loop** для обработки элементов.
   * Потоки обращаются к данным не по соседству, что снижает эффективность доступа к глобальной памяти.

   **b. Коалесцированный доступ (Coalesced)**

   * Прямое отображение `tid -> i`, где каждый поток обрабатывает соседние элементы массива.
   * Доступ к глобальной памяти максимально эффективный, потокам проще считывать смежные данные одновременно.

   **c. Коалесцированный доступ с использованием Shared Memory**

   * Блоки потоков загружают данные в **разделяемую память (`shared memory`)**, выполняют вычисления и записывают результаты обратно в глобальную память.
   * Уменьшает количество обращений к медленной глобальной памяти и повышает производительность.

4. **Настройка конфигурации CUDA**

   * Тестируются разные размеры блоков: `64, 128, 256, 512`.
   * Количество блоков рассчитывается как `(N + threads_per_block - 1) / threads_per_block`.
   * Для точного измерения времени используется `cudaEvent_t`.

5. **Замер производительности и статистика**

   * Для каждого ядра измеряется **время выполнения**.
   * Результаты проверяются на корректность: вычисляется **сумма, среднее и дисперсия** выходного массива.

Пример вывода для массива из 10 элементов и блока из 128 потоков:

```
Kernel: Coalesced (Global Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.021

Kernel: Non-Coalesced (Global Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.035

Kernel: Coalesced (Shared Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.018
```
6. **Особенности и производительность**

* **Коалесцированный доступ** показывает лучшую производительность по сравнению с некоалесцированным.
* Использование **shared memory** дополнительно ускоряет вычисления за счёт сокращения обращений к глобальной памяти.
* Размер блока и организация потоков влияют на эффективность распределения вычислений.
* Проверка результатов на CPU подтверждает корректность работы всех ядер.

**Выводы по заданию 2:**

1. Эффективный (коалесцированный) доступ к глобальной памяти критически важен для ускорения работы GPU.
2. Использование shared memory значительно улучшает производительность на больших массивах.
3. Неправильная организация потоков или некоалесцированный доступ может приводить к падению производительности до нескольких раз.
4. Оптимизация доступа к памяти совместно с правильной конфигурацией блоков потоков позволяет максимально использовать потенциал GPU.

### Результаты выполнения задания 2 (полные данные)

| Array Size | Threads/Block | Kernel                        | Sum | Mean | Variance | Time (ms) |
| ---------- | ------------- | ----------------------------- | --- | ---- | -------- | --------- |
| 10         | 64            | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.128608  |
| 10         | 64            | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.031680  |
| 10         | 64            | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.028896  |
| 10         | 128           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.017536  |
| 10         | 128           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.015936  |
| 10         | 128           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014976  |
| 10         | 256           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.014944  |
| 10         | 256           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.014560  |
| 10         | 256           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014976  |
| 10         | 512           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.014208  |
| 10         | 512           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.014656  |
| 10         | 512           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014176  |
| 100 | 64 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.016384 |
| 100 | 64 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.016160 |
| 100 | 64 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014208 |
| 100 | 128 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014432 |
| 100 | 128 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.015328 |
| 100 | 128 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014592 |
| 100 | 256 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014880 |
| 100 | 256 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014688 |
| 100 | 256 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.017376 |
| 100 | 512 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.013440 |
| 100 | 512 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014144 |
| 100 | 512 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014688 |
| 1000 | 64 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.016352 |
| 1000 | 64 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.016512 |
| 1000 | 64 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.015776 |
| 1000 | 128 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015232 |
| 1000 | 128 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015232 |
| 1000 | 128 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.014816 |
| 1000 | 256 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015328 |
| 1000 | 256 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015488 |
| 1000 | 256 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.013920 |
| 1000 | 512 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.014912 |
| 1000 | 512 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015744 |
| 1000 | 512 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.015168 |
| 10000 | 64 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.033408 |
| 10000 | 64 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.019584 |
| 10000 | 64 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.018624 |
| 10000 | 128 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015648 |
| 10000 | 128 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.018016 |
| 10000 | 128 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.017088 |
| 10000 | 256 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015680 |
| 10000 | 256 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015296 |
| 10000 | 256 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.015232 |
| 10000 | 512 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.016672 |
| 10000 | 512 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.016928 |
| 10000 | 512 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.015424 |
| 100000 | 64 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.024288 |
| 100000 | 64 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.024704 |
| 100000 | 64 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.024288 |
| 100000 | 128 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.021248 |
| 100000 | 128 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.022272 |
| 100000 | 128 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021344 |
| 100000 | 256 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.020480 |
| 100000 | 256 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.023040 |
| 100000 | 256 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021888 |
| 100000 | 512 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.020928 |
| 100000 | 512 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.022912 |
| 100000 | 512 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021760 |
| 1000000 | 64 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.061664 |
| 1000000 | 64 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.067520 |
| 1000000 | 64 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.065056 |
| 1000000 | 128 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.051072 |
| 1000000 | 128 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.070816 |
| 1000000 | 128 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.066112 |
| 1000000 | 256 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.055520 |
| 1000000 | 256 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.065216 |
| 1000000 | 256 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.058304 |
| 1000000 | 512 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.053184 |
| 1000000 | 512 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.063456 |
| 1000000 | 512 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.057344 |
| 10000000 | 64 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.423616 |
| 10000000 | 64 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.470240 |
| 10000000 | 64 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.451552 |
| 10000000 | 128 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.360864 |
| 10000000 | 128 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.439456 |
| 10000000 | 128 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.396224 |
| 10000000 | 256 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.381088 |
| 10000000 | 256 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.448288 |
| 10000000 | 256 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.402912 |
| 10000000 | 512 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.383040 |
| 10000000 | 512 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.484384 |
| 10000000 | 512 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.442880 |
| 100000000 | 64 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.03139 |
| 100000000 | 64 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.37059 |
| 100000000 | 64 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 4.19834 |
| 100000000 | 128 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.35312 |
| 100000000 | 128 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.07840 |
| 100000000 | 128 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 3.66762 |
| 100000000 | 256 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.36768 |
| 100000000 | 256 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.16147 |
| 100000000 | 256 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 3.73946 |
| 100000000 | 512 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.55792 |
| 100000000 | 512 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.54509 |
| 100000000 | 512 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 4.00995 |

## Задание 3: Профилирование гибридного приложения CPU + GPU

**Цель:**
Разработать гибридную программу, в которой часть вычислений выполняется на CPU, а часть — на GPU, и провести детальное профилирование.

**Описание реализации:**

* CPU вычисляет сумму и среднее значение массива.
* GPU вычисляет дисперсию массива.
* Используются:

  * `cudaMemcpyAsync`,
  * `cudaStream`,
  * CUDA events для раздельного измерения времени.

**Профилируемые этапы GPU:**

* Host → Device (H2D) copy,
* выполнение CUDA-ядра,
* Device → Host (D2H) copy.

**Оптимизация:**

* Реализовано использование **pinned (page-locked) memory**:

  ```cpp
  cudaMallocHost(&h_data, N * sizeof(float));
  ```
* Это позволило снизить накладные расходы передачи данных.

**Пример результата профилирования:**

```
CPU time (s):        0.000140
H2D time (s):        0.000027
Kernel time (s):     0.000061
D2H time (s):        0.000016
GPU total time (s):  0.000104
Hybrid total time (s): 0.000244
```

**Выводы:**

* Основные накладные расходы приходятся на передачу данных.
* Использование pinned memory уменьшает время H2D и D2H копирования.
* Для небольших массивов CPU быстрее, для больших — GPU даёт выигрыш.

---

## Задание 4: Анализ масштабируемости распределённой программы (MPI)

**Цель:**
Оценить **strong scaling** и **weak scaling** MPI-программы.

**Описание:**

* Реализовано вычисление агрегатной функции над массивом.
* Использованы `MPI_Reduce` и `MPI_Allreduce`.
* Выполнены замеры времени при различном числе процессов.

**Анализ:**

* Strong scaling: фиксированный размер задачи.
* Weak scaling: размер задачи растёт пропорционально числу процессов.

**Выводы:**

* Коммуникационные операции ограничивают масштабируемость.
* MPI эффективен для больших задач и умеренного числа процессов.

---

## Контрольные вопросы

Ответы на контрольные вопросы приведены в файле **questions.md**.

---

## Заключение

В ходе практической работы были:

* изучены методы профилирования CPU, GPU и распределённых программ;
* реализованы и оптимизированы параллельные и гибридные алгоритмы;
* проанализированы накладные расходы и узкие места;
* продемонстрировано влияние архитектуры и памяти на производительность.

Практическая работа дала целостное понимание **HPC-подходов и гибридных вычислений**, а также показала важность профилирования при разработке эффективных параллельных программ.

