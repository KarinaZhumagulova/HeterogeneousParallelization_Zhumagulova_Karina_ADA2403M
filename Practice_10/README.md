# Практическая работа №10: Профилирование и оптимизация параллельных и гибридных приложений

**Astana IT University**

**Курс:** Heterogeneous Parallelization

**Преподаватель:** Садвакасова Куралай Жанжигитовна

**Студент:** Жумагулова Карина

**Группа:** ADA-2403M

**Дата:** 25.01.2026

## Краткое описание практической работы

Данная практическая работа посвящена **анализу производительности, профилированию и оптимизации параллельных программ**, реализованных с использованием технологий:

* **OpenMP** (CPU-параллелизм),
* **CUDA** (GPU-вычисления),
* **MPI** (распределённые вычисления),
* **гибридных CPU + GPU решений**.

Основная цель работы — изучить, **как архитектура вычислений, доступ к памяти и коммуникации между CPU и GPU влияют на производительность**, а также научиться выявлять узкие места и применять оптимизации.

В рамках работы реализованы четыре задания:

1. Анализ производительности CPU-параллельной программы (OpenMP).
2. Анализ и оптимизация доступа к памяти на GPU (CUDA).
3. Профилирование гибридного приложения CPU + GPU.
4. Анализ масштабируемости распределённой программы (MPI).

Для всех заданий:

* выполнены **замеры времени выполнения**,
* проведено **профилирование отдельных этапов вычислений**,
* сделаны выводы о масштабируемости и узких местах.

## Цель работы

* Изучить методы **профилирования параллельных программ**.
* Проанализировать влияние числа потоков и процессов на ускорение.
* Исследовать **накладные расходы передачи данных** между CPU и GPU.
* Освоить оптимизации памяти (coalesced access, shared memory, pinned memory).
* Проанализировать масштабируемость с точки зрения **законов Амдала и Густафсона**.

## Структура репозитория

```
.
├── task_1_openmp.cpp        # OpenMP: анализ CPU-параллелизма
├── task_2_cuda_memory.cu    # CUDA: доступ к памяти и оптимизация
├── task_3_hybrid.cu         # Гибридное CPU + GPU приложение
├── task_4_mpi.cpp           # MPI: strong и weak scaling
├── questions.md             # Ответы на контрольные вопросы
├── README.md                # Этот файл
```

## Теоретическая часть

### Профилирование параллельных программ

**Профилирование** — это процесс измерения:

* времени выполнения отдельных частей программы,
* загрузки вычислительных ресурсов,
* накладных расходов синхронизации и передачи данных.

В данной работе используются:

* `omp_get_wtime()` — для CPU (OpenMP),
* `cudaEventRecord()` — для GPU,
* `MPI_Wtime()` — для распределённых программ.

## Практическая часть

## Задание 1: Анализ производительности CPU-параллельной программы (OpenMP)

### Цель

Реализовать параллельное вычисление **суммы, среднего значения и дисперсии** элементов большого массива на CPU с использованием OpenMP.
Сравнить **последовательную** и **параллельную** части программы, оценить **ускорение** при увеличении числа потоков и проанализировать результаты в контексте закона Амдала.

### Методика

1. **Генерация данных**

   * Создаётся массив случайных чисел целого типа (диапазон `-100 … 100`) размера `N`.
   * Для наглядности выводятся **первые и последние 10 элементов массива**.

2. **Последовательная часть**

   * Заполнение массива случайными числами выполняется **одним потоком**.
   * Время засекалось с помощью `omp_get_wtime()`.

3. **Параллельная часть**

   * Вычисление **суммы элементов** с использованием директивы OpenMP:

     ```cpp
     #pragma omp parallel for reduction(+:sum)
     ```
   * Вычисление **среднего**: `mean = sum / N`.
   * Вычисление **дисперсии** с помощью параллельного цикла и редукции:

     ```cpp
     #pragma omp parallel for reduction(+:variance)
     ```
   * Время выполнения замерялось отдельным таймером `omp_get_wtime()`.

4. **Метрики**

   * Сумма (`sum`), среднее (`mean`) и дисперсия (`variance`) проверяются на корректность.
   * Определяется доля последовательной и параллельной частей в общем времени.
   * Анализ ускорения при разном количестве потоков: 1, 2, 4, 8.

### 1. Доля параллельной и последовательной частей программы

**Что видно из данных:**

* Последовательная часть (`Sequential time`) — это **генерация массива случайных чисел** и вывод краёв массива.
* Параллельная часть (`Parallel time`) — это **вычисление суммы, среднего и дисперсии через OpenMP**.

**Наблюдения:**

| Размер массива                   | Доля последовательной части | Доля параллельной части |
| -------------------------------- | --------------------------- | ----------------------- |
| Малые (10–1000)                  | 0.4–0.9                     | 0.1–0.6                 |
| Средние (10 000–1 000 000)       | 0.85–0.95                   | 0.05–0.15               |
| Большие (10 000 000–100 000 000) | 0.90–0.97                   | 0.03–0.10               |

**Выводы:**

* Для **малых массивов** параллельная часть занимает значительную долю (иногда до 60%).
* Для **больших массивов** почти 90–97% времени уходит на **последовательную генерацию данных**, и параллельные вычисления занимают только 3–10%.
* Это объясняет, почему ускорение с ростом потоков не такое большое — последовательная часть сильно ограничивает общую производительность.

### 2. Влияние числа потоков на ускорение

Ускорение можно оценить по формуле:

[
S = \frac{T_1}{T_p}
]

где (T_1) — общее время на 1 потоке, (T_p) — общее время на p потоках.

**Примеры для больших массивов (100 000 000):**

| Потоки | Total time (s) | Ускорение (S = T_1 / T_p) |
| ------ | -------------- | ------------------------- |
| 1      | 11.744         | 1.00                      |
| 2      | 10.683         | 1.10                      |
| 4      | 10.957         | 1.07                      |
| 8      | 10.129         | 1.16                      |

**Выводы:**

* Для **больших массивов** ускорение крайне слабое (~1.1–1.16x), хотя потоков 8.
* Для **малых массивов** ускорение иногда даже хуже, из-за накладных расходов на создание потоков и синхронизацию.

### 3.  Закон Амдала

**Формула закона Амдала:**

[
S_\text{max} = \frac{1}{(1-P) + \frac{P}{N}}
]

где:

* (P) — доля параллельной части,
* (N) — число потоков,
* (1-P) — последовательная часть.

**Пример для массива 100 000 000, 8 потоков:**

* Параллельная доля (P \approx 0.065) (среднее из таблицы)
* Последовательная часть (1-P \approx 0.935)

[
S_\text{max} = \frac{1}{0.935 + 0.065/8} \approx \frac{1}{0.9431} \approx 1.06
]

**Сравнение с фактическим ускорением:** ~1.16 — близко к расчету (малые расхождения из-за измерений и генерации случайных чисел).

**Выводы по закону Амдала:**

* Ускорение ограничено высокой последовательной частью.
* Даже при увеличении потоков выше 8–16 реального ускорения почти не будет.
* Для улучшения ускорения нужно уменьшить последовательную часть — например, **сделать генерацию случайных чисел тоже параллельной**.

### 4. Итоговые конкретные выводы

1. **Доля последовательной/параллельной частей:**

   * Малые массивы: параллельная часть ~50%
   * Большие массивы: параллельная часть 3–10%, последовательная 90–97%

2. **Влияние числа потоков на ускорение:**

   * С ростом потоков ускорение увеличивается очень слабо для больших массивов.
   * Для малых массивов накладные расходы на OpenMP уменьшают эффективность.

3. **Закон Амдала:**

   * Ограничивает максимальное ускорение из-за высокой доли последовательной части.
   * Для массивов 100 млн элементов, с 8 потоками, теоретическое ускорение ~1.06x.

4. **Рекомендации для оптимизации:**

   * Параллелить генерацию случайных чисел (`#pragma omp parallel for`)
   * Параллельно выводить данные или избегать печати для больших массивов
   * Сосредоточиться на больших вычислительных блоках для OpenMP

### Платформа

* Язык: C++
* Параллельная модель: OpenMP
* Замер времени: `omp_get_wtime()`
* Диапазон значений: `[-100, 100]`
* Количество потоков: `1, 2, 4, 8`

### Результаты экспериментов

1. **Корректность**: все методы дают правильные значения суммы, среднего и дисперсии.
2. **Доля последовательной части**: при заполнении массива и генерации случайных чисел последовательная часть занимает большую часть времени для малых массивов.
3. **Ускорение при параллельных вычислениях**: с ростом числа потоков время параллельной части уменьшается, но влияние последовательной части ограничивает ускорение (закон Амдала).
4. **Масштабируемость**: для больших массивов ускорение становится более заметным.
5. **Рекомендация**: для оптимизации важно минимизировать последовательные участки и эффективно распределять нагрузку между потоками.

#### OpenMP threads = 1

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 1.19e-06            | 1.58e-04          | 1.59e-04       | 0.0075              | 0.9925            |
| 100            | 9.06e-06            | 1.50e-05          | 2.41e-05       | 0.3762              | 0.6238            |
| 1 000          | 1.22e-04            | 1.72e-05          | 1.39e-04       | 0.8767              | 0.1233            |
| 10 000         | 1.32e-03            | 1.41e-04          | 1.46e-03       | 0.9032              | 0.0968            |
| 100 000        | 1.12e-02            | 2.05e-03          | 1.33e-02       | 0.8454              | 0.1546            |
| 1 000 000      | 1.04e-01            | 1.30e-02          | 1.17e-01       | 0.8885              | 0.1115            |
| 10 000 000     | 1.18                | 0.130             | 1.31           | 0.9002              | 0.0998            |
| 100 000 000    | 9.99                | 1.75              | 11.74          | 0.8510              | 0.1490            |

#### OpenMP threads = 2

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 9.54e-07            | 7.97e-04          | 7.98e-04       | 0.0012              | 0.9988            |
| 100            | 7.15e-06            | 2.60e-05          | 3.31e-05       | 0.2158              | 0.7842            |
| 1 000          | 8.51e-05            | 8.70e-05          | 1.72e-04       | 0.4945              | 0.5055            |
| 10 000         | 1.00e-03            | 2.28e-04          | 1.23e-03       | 0.8147              | 0.1853            |
| 100 000        | 1.04e-02            | 9.88e-04          | 1.14e-02       | 0.9136              | 0.0864            |
| 1 000 000      | 9.44e-02            | 7.15e-03          | 0.102          | 0.9296              | 0.0704            |
| 10 000 000     | 0.945               | 0.068             | 1.01           | 0.9327              | 0.0673            |
| 100 000 000    | 9.47                | 1.22              | 10.68          | 0.8862              | 0.1138            |

#### OpenMP threads = 4

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 9.54e-07            | 3.44e-04          | 3.45e-04       | 0.0028              | 0.9972            |
| 100            | 3.79e-05            | 1.17e-04          | 1.55e-04       | 0.2446              | 0.7554            |
| 1 000          | 8.61e-05            | 1.26e-04          | 2.12e-04       | 0.4056              | 0.5944            |
| 10 000         | 1.07e-03            | 1.78e-04          | 1.24e-03       | 0.8570              | 0.1430            |
| 100 000        | 9.16e-03            | 7.55e-04          | 9.91e-03       | 0.9238              | 0.0762            |
| 1 000 000      | 9.12e-02            | 4.40e-03          | 0.0956         | 0.9540              | 0.0460            |
| 10 000 000     | 0.931               | 0.043             | 0.974          | 0.9559              | 0.0441            |
| 100 000 000    | 10.07               | 0.886             | 10.96          | 0.9192              | 0.0808            |

#### OpenMP threads = 8

| Размер массива | Sequential time (s) | Parallel time (s) | Total time (s) | Sequential fraction | Parallel fraction |
| -------------- | ------------------- | ----------------- | -------------- | ------------------- | ----------------- |
| 10             | 1.91e-06            | 7.36e-04          | 7.38e-04       | 0.0026              | 0.9974            |
| 100            | 1.10e-05            | 5.61e-04          | 5.72e-04       | 0.0192              | 0.9808            |
| 1 000          | 2.77e-04            | 4.18e-04          | 6.95e-04       | 0.3986              | 0.6014            |
| 10 000         | 2.78e-03            | 4.58e-04          | 3.24e-03       | 0.8585              | 0.1415            |
| 100 000        | 1.35e-02            | 1.13e-03          | 1.47e-02       | 0.9227              | 0.0773            |
| 1 000 000      | 0.133               | 3.69e-03          | 0.137          | 0.9730              | 0.0270            |
| 10 000 000     | 1.40                | 0.034             | 1.43           | 0.9762              | 0.0238            |
| 100 000 000    | 9.47                | 0.658             | 10.13          | 0.9351              | 0.0649            |

Скриншот результатов (частично)
<img width="940" height="738" alt="image" src="https://github.com/user-attachments/assets/69163ee0-6cbe-4bf3-b123-852a8583a351" />

## Задание 2: Оптимизация доступа к памяти на GPU (CUDA)

**Цель:**
Реализовать ядро CUDA для обработки массива данных, продемонстрировать влияние разных паттернов доступа к памяти на производительность и оптимизировать доступ с использованием разделяемой памяти.

**Описание реализации:**

1. **Генерация данных на CPU**

   * Создаётся массив `h_data` размером `N` элементов типа `float`.
   * Диапазон случайных чисел: от -100 до 100.
   * Для контроля данных выводятся **первые 10 элементов массива**:

```
First 10 elements: 23 -45 12 0 89 -3 67 12 5 -98
```
2. **Выделение памяти на GPU**

   * Выделяется глобальная память для исходного массива `d_data` и массива результатов `d_output`.
   * Данные копируются с CPU на GPU с помощью `cudaMemcpy`.

3. **Реализация ядер CUDA**

   **a. Некоалесцированный доступ (Non-Coalesced)**

   * Используется **grid-stride loop** для обработки элементов.
   * Потоки обращаются к данным не по соседству, что снижает эффективность доступа к глобальной памяти.

   **b. Коалесцированный доступ (Coalesced)**

   * Прямое отображение `tid -> i`, где каждый поток обрабатывает соседние элементы массива.
   * Доступ к глобальной памяти максимально эффективный, потокам проще считывать смежные данные одновременно.

   **c. Коалесцированный доступ с использованием Shared Memory**

   * Блоки потоков загружают данные в **разделяемую память (`shared memory`)**, выполняют вычисления и записывают результаты обратно в глобальную память.
   * Уменьшает количество обращений к медленной глобальной памяти и повышает производительность.

4. **Настройка конфигурации CUDA**

   * Тестируются разные размеры блоков: `64, 128, 256, 512`.
   * Количество блоков рассчитывается как `(N + threads_per_block - 1) / threads_per_block`.
   * Для точного измерения времени используется `cudaEvent_t`.

5. **Замер производительности и статистика**

   * Для каждого ядра измеряется **время выполнения**.
   * Результаты проверяются на корректность: вычисляется **сумма, среднее и дисперсия** выходного массива.

Пример вывода для массива из 10 элементов и блока из 128 потоков:

```
Kernel: Coalesced (Global Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.021

Kernel: Non-Coalesced (Global Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.035

Kernel: Coalesced (Shared Memory)
Threads/block: 128
Sum: 47
Mean: 4.7
Variance: 123.25
Time (ms): 0.018
```
6. **Особенности и производительность**

* **Коалесцированный доступ** показывает лучшую производительность по сравнению с некоалесцированным.
* Использование **shared memory** дополнительно ускоряет вычисления за счёт сокращения обращений к глобальной памяти.
* Размер блока и организация потоков влияют на эффективность распределения вычислений.
* Проверка результатов на CPU подтверждает корректность работы всех ядер.

**Выводы по заданию 2:**

1. Эффективный (коалесцированный) доступ к глобальной памяти критически важен для ускорения работы GPU.
2. Использование shared memory значительно улучшает производительность на больших массивах.
3. Неправильная организация потоков или некоалесцированный доступ может приводить к падению производительности до нескольких раз.
4. Оптимизация доступа к памяти совместно с правильной конфигурацией блоков потоков позволяет максимально использовать потенциал GPU.
   
### Результаты выполнения задания 2 (полные данные)
1. Все методы дают одинаковые значения статистик, что подтверждает корректность реализации.
2. Coalesced доступ к глобальной памяти быстрее некоалесцированного, особенно при больших массивах.
3. Использование Shared Memory даёт дополнительное ускорение для малых и средних размеров блоков.
4. Увеличение числа потоков на блок улучшает производительность до определённого предела, после чего эффект уменьшается (нагрузка на GPU и накладные расходы на синхронизацию).
5. Для очень больших массивов время выполнения измеряется в сотых долях секунды, что демонстрирует эффективность параллельной обработки на GPU.

| Array Size | Threads/Block | Kernel                        | Sum | Mean | Variance | Time (ms) |
| ---------- | ------------- | ----------------------------- | --- | ---- | -------- | --------- |
| 10         | 64            | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.128608  |
| 10         | 64            | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.031680  |
| 10         | 64            | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.028896  |
| 10         | 128           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.017536  |
| 10         | 128           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.015936  |
| 10         | 128           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014976  |
| 10         | 256           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.014944  |
| 10         | 256           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.014560  |
| 10         | 256           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014976  |
| 10         | 512           | Coalesced (Global Memory)     | 56  | 5.6  | 19542.2  | 0.014208  |
| 10         | 512           | Non-Coalesced (Global Memory) | 56  | 5.6  | 19542.2  | 0.014656  |
| 10         | 512           | Coalesced (Shared Memory)     | 56  | 5.6  | 19542.2  | 0.014176  |
| 100 | 64 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.016384 |
| 100 | 64 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.016160 |
| 100 | 64 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014208 |
| 100 | 128 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014432 |
| 100 | 128 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.015328 |
| 100 | 128 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014592 |
| 100 | 256 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014880 |
| 100 | 256 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014688 |
| 100 | 256 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.017376 |
| 100 | 512 | Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.013440 |
| 100 | 512 | Non-Coalesced (Global Memory) | -912 | -9.12 | 11743.4 | 0.014144 |
| 100 | 512 | Coalesced (Shared Memory) | -912 | -9.12 | 11743.4 | 0.014688 |
| 1000 | 64 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.016352 |
| 1000 | 64 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.016512 |
| 1000 | 64 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.015776 |
| 1000 | 128 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015232 |
| 1000 | 128 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015232 |
| 1000 | 128 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.014816 |
| 1000 | 256 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015328 |
| 1000 | 256 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015488 |
| 1000 | 256 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.013920 |
| 1000 | 512 | Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.014912 |
| 1000 | 512 | Non-Coalesced (Global Memory) | 2708 | 2.708 | 13603 | 0.015744 |
| 1000 | 512 | Coalesced (Shared Memory) | 2708 | 2.708 | 13603 | 0.015168 |
| 10000 | 64 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.033408 |
| 10000 | 64 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.019584 |
| 10000 | 64 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.018624 |
| 10000 | 128 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015648 |
| 10000 | 128 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.018016 |
| 10000 | 128 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.017088 |
| 10000 | 256 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015680 |
| 10000 | 256 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.015296 |
| 10000 | 256 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.015232 |
| 10000 | 512 | Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.016672 |
| 10000 | 512 | Non-Coalesced (Global Memory) | -1552 | -0.1552 | 13651.4 | 0.016928 |
| 10000 | 512 | Coalesced (Shared Memory) | -1552 | -0.1552 | 13651.4 | 0.015424 |
| 100000 | 64 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.024288 |
| 100000 | 64 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.024704 |
| 100000 | 64 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.024288 |
| 100000 | 128 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.021248 |
| 100000 | 128 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.022272 |
| 100000 | 128 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021344 |
| 100000 | 256 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.020480 |
| 100000 | 256 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.023040 |
| 100000 | 256 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021888 |
| 100000 | 512 | Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.020928 |
| 100000 | 512 | Non-Coalesced (Global Memory) | 19254 | 0.19254 | 13510 | 0.022912 |
| 100000 | 512 | Coalesced (Shared Memory) | 19254 | 0.19254 | 13510 | 0.021760 |
| 1000000 | 64 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.061664 |
| 1000000 | 64 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.067520 |
| 1000000 | 64 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.065056 |
| 1000000 | 128 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.051072 |
| 1000000 | 128 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.070816 |
| 1000000 | 128 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.066112 |
| 1000000 | 256 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.055520 |
| 1000000 | 256 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.065216 |
| 1000000 | 256 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.058304 |
| 1000000 | 512 | Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.053184 |
| 1000000 | 512 | Non-Coalesced (Global Memory) | 21514 | 0.021514 | 13469.8 | 0.063456 |
| 1000000 | 512 | Coalesced (Shared Memory) | 21514 | 0.021514 | 13469.8 | 0.057344 |
| 10000000 | 64 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.423616 |
| 10000000 | 64 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.470240 |
| 10000000 | 64 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.451552 |
| 10000000 | 128 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.360864 |
| 10000000 | 128 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.439456 |
| 10000000 | 128 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.396224 |
| 10000000 | 256 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.381088 |
| 10000000 | 256 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.448288 |
| 10000000 | 256 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.402912 |
| 10000000 | 512 | Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.383040 |
| 10000000 | 512 | Non-Coalesced (Global Memory) | -286368 | -0.0286368 | 13368.3 | 0.484384 |
| 10000000 | 512 | Coalesced (Shared Memory) | -286368 | -0.0286368 | 13368.3 | 0.442880 |
| 100000000 | 64 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.03139 |
| 100000000 | 64 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.37059 |
| 100000000 | 64 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 4.19834 |
| 100000000 | 128 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.35312 |
| 100000000 | 128 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.07840 |
| 100000000 | 128 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 3.66762 |
| 100000000 | 256 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.36768 |
| 100000000 | 256 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.16147 |
| 100000000 | 256 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 3.73946 |
| 100000000 | 512 | Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 3.55792 |
| 100000000 | 512 | Non-Coalesced (Global Memory) | -98180 | -0.0009818 | 9127.68 | 4.54509 |
| 100000000 | 512 | Coalesced (Shared Memory) | -98180 | -0.0009818 | 9127.68 | 4.00995 |

<img width="940" height="852" alt="image" src="https://github.com/user-attachments/assets/3800caf7-dca8-4706-ab4d-f89bd01c75ff" />

## Задание 3: Профилирование гибридного приложения CPU + GPU

### Цель

Реализовать **гибридное вычисление статистик массива данных**, где:

* Часть вычислений выполняется на **CPU** (сумма и среднее значение),
* Часть — на **GPU** (вычисление дисперсии через ядро CUDA).

Провести **профилирование** приложения и выявить узкие места при взаимодействии CPU и GPU.

### Методика

1. **Гибридный алгоритм**

   * **CPU**: вычисление суммы элементов и среднего значения массива.
   * **GPU**: расчет дисперсии (суммы квадратов отклонений) через ядро `kernel_variance`.
   * **Используются CUDA Streams** для асинхронных операций:

     * `cudaMemcpyAsync` для копирования H2D (Host → Device) и D2H (Device → Host).

2. **Профилирование**

   * Для каждой операции GPU фиксируются **события CUDA (`cudaEvent_t`)**:

     * H2D: копирование данных на GPU
     * Kernel: выполнение ядра
     * D2H: возврат результата на CPU
   * Время CPU измеряется через `omp_get_wtime()`.

3. **Оптимизация**

   * Используется **pinned memory** (`cudaMallocHost`) для ускорения передачи данных между CPU и GPU.
   * Использование **stream** позволяет выполнять копирование и вычисления **асинхронно**, минимизируя простои.

4. **Метрики**

   * Сумма (`sum`), среднее (`mean`) и дисперсия (`variance`) проверяются на корректность.
   * Время CPU, H2D, Kernel, D2H и общее время приложения (`Hybrid total time`) фиксируются для анализа накладных расходов.

### Результаты

| Array Size  | Sum    | Mean     | Variance | CPU time (s) | H2D time (s) | Kernel time (s) | D2H time (s) | GPU total (s) | Hybrid total (s) |
| ----------- | ------ | -------- | -------- | ------------ | ------------ | --------------- | ------------ | ------------- | ---------------- |
| 10          | -391   | -39.100  | 1592.490 | 0.000001     | 0.000010     | 0.000096        | 0.000023     | 0.000129      | 0.000130         |
| 100         | -115   | -1.150   | 3112.008 | 0.000001     | 0.000012     | 0.000010        | 0.000019     | 0.000041      | 0.000042         |
| 1 000       | 392    | 0.392    | 3440.353 | 0.000005     | 0.000012     | 0.000012        | 0.000014     | 0.000038      | 0.000043         |
| 10 000      | 5004   | 0.5004   | 3340.129 | 0.000034     | 0.000016     | 0.000045        | 0.000018     | 0.000079      | 0.000114         |
| 100 000     | -5963  | -0.05963 | 3355.096 | 0.000307     | 0.000049     | 0.000362        | 0.000012     | 0.000423      | 0.000730         |
| 1 000 000   | 58425  | 0.058425 | 3363.681 | 0.003009     | 0.000396     | 0.003521        | 0.000016     | 0.003933      | 0.006942         |
| 10 000 000  | 327918 | 0.032792 | 3342.116 | 0.029106     | 0.003475     | 0.035098        | 0.000022     | 0.038596      | 0.067702         |
| 100 000 000 | 650980 | 0.006510 | 2282.124 | 0.300319     | 0.033095     | 0.320033        | 0.000039     | 0.353166      | 0.653485         |

> Время GPU указано в **секундах**, переведено из миллисекунд.
<img width="2150" height="456" alt="image" src="https://github.com/user-attachments/assets/240b6ef0-3c8a-411a-81d0-93cbb45baa89" />

### Анализ

1. **Накладные расходы передачи данных**

   * Для малых массивов (`N < 1000`) H2D и D2H занимают больше времени, чем вычисления на GPU.
   * Для больших массивов основная доля времени уходит на **копирование на GPU** и обратно.

2. **Узкие места**

   * Основной узкий участок: **асинхронные копирования H2D и D2H** для больших массивов.
   * Ядро CUDA работает быстрее, чем передачa данных, что показывает **узкий канал PCIe**.

3. **Оптимизация**

   * Использование **pinned memory** и **stream** позволяет асинхронно копировать данные и запускать ядро, снижая время ожидания CPU.
   * Для дальнейшей оптимизации можно **делить массив на чанки** и использовать **overlap** копирования и вычислений (double-buffering).

4. **Выводы**

<img width="1189" height="690" alt="image" src="https://github.com/user-attachments/assets/6ac4962d-08cc-48cc-b90a-ac89d73ace7b" />

1. **Влияние размера массива на время выполнения**

   * Для малых массивов (N ≤ 1000) общее время почти полностью определяется **CPU**, H2D и Kernel выполняются очень быстро.
   * Для больших массивов (N ≥ 10⁶) основная часть времени приходится на **Kernel** и копирование данных на GPU (H2D). CPU-сумма/среднее становится незначительной долей.

2. **Накладные расходы передачи данных**

   * H2D и D2H занимают заметное время для больших массивов.
   * D2H (Device→Host) обычно меньше H2D, так как копируется только результат (одна переменная), а не весь массив.
   * Следовательно, **узким местом** является именно копирование массива Host→Device при больших объемах данных.

3. **Эффект асинхронной передачи и CUDA Streams**

   * Использование `cudaMemcpyAsync` с потоками позволяет перекрывать передачу данных с вычислениями на GPU, но для маленьких массивов накладные расходы на создание потоков и событий могут быть больше, чем выигрыш.

4. **Эффект оптимизации**

   * Оптимизация, которая уменьшает H2D (например, использование pinned memory, объединение нескольких операций копирования в один вызов, или обработка блоков данных по частям), сильно снижает общий hybrid total time.
   * В нашем коде уже применена **pinned memory**, что заметно ускоряет H2D для больших массивов.

5. **Закон Амдала**

   * Для малых массивов ускорение ограничено долей параллельного кода: CPU-сумма/среднее остаются последовательными, поэтому эффективность GPU невысока.
   * Для больших массивов параллельная часть (Kernel) доминирует, и эффект ускорения значительный, но накладные расходы на передачу данных и синхронизацию лимитируют дальнейшее ускорение.

6. **Практические рекомендации**

   * Для массивов до ~10⁵ элементов CPU+GPU гибрид не имеет большого смысла; проще использовать CPU.
   * Для массивов ≥10⁶ элементов GPU дает преимущество, но важно **сокращать копирование H2D**, например:

     * обрабатывать данные пакетами (chunking),
     * использовать **page-locked (pinned) memory**,
     * уменьшать количество синхронизаций и событий.

### **Задание 4: Анализ масштабируемости распределённой программы (MPI)**
#### **Описание задания**

Цель задания — исследовать масштабируемость распределённой программы, реализованной с использованием MPI (Message Passing Interface). Программа выполняет вычисление агрегатной функции (суммы, среднего значения и дисперсии) для большого массива данных.

**Основные требования:**

1. Реализовать MPI-программу, которая делит массив между процессами и вычисляет:

   * Локальные суммы и дисперсию;
   * Глобальные значения с помощью `MPI_Reduce` и `MPI_Allreduce`.
2. Измерить время выполнения при различном числе процессов.
3. Проанализировать:

   * Strong scaling (фиксированный размер массива, растущее число процессов);
   * Weak scaling (рост массива пропорционально числу процессов).
4. Сделать выводы о масштабируемости алгоритма и его практических ограничениях.

#### **Описание программы**

* Каждый процесс получает свою локальную порцию массива (`local_data`), размер которой зависит от числа процессов и общего размера массива.
* На каждом процессе вычисляется **локальная сумма** и **локальная сумма квадратов отклонений от среднего**.
* Для получения глобальной суммы используется `MPI_Reduce`, а для дисперсии — `MPI_Allreduce`.
* Программа выводит:

  * Первые 10 элементов локальной порции (для контроля данных);
  * Глобальную сумму, среднее и дисперсию;
  * Общее время выполнения для данной конфигурации.

#### **Методика измерений**

1. Используется `MPI_Wtime()` для замера времени выполнения.
2. Замер включает:

   * Генерацию локальных данных;
   * Вычисление локальных сумм и дисперсии;
   * Коммуникацию (`MPI_Reduce`, `MPI_Allreduce`).
3. Выполняется для разных размеров массивов и числа процессов:

   * Процессы: 1, 2, 4, 8, 16, 32
   * Массивы: 10, 100, 1 000, 10 000, 100 000, 1 000 000, 10 000 000, 100 000 000

#### **Результаты**

| Array size | Processes | Sum     | Mean        | Variance | Total time (s) |
| ---------- | --------- | ------- | ----------- | -------- | -------------- |
| 10         | 1         | -195    | -19.5       | 1255.85  | 0.000201       |
| 100        | 1         | 426     | 4.26        | 3164.63  | 0.000004       |
| 1000       | 1         | 203     | 0.203       | 3349.13  | 0.000026       |
| 10000      | 1         | -6354   | -0.6354     | 3393.73  | 0.000369       |
| 100000     | 1         | 7189    | 0.07189     | 3375.10  | 0.005094       |
| 1000000    | 1         | 58542   | 0.058542    | 3363.51  | 0.049851       |
| 10000000   | 1         | 364646  | 0.0364646   | 3366.80  | 0.513072       |
| 100000000  | 1         | 110463  | 0.00110463  | 3366.49  | 6.76946        |
| 10         | 2         | -330    | -33         | 1589     | 0.000647       |
| 100        | 2         | 1776    | 17.76       | 3388.54  | 0.000032       |
| 1000       | 2         | 1619    | 1.619       | 3390.58  | 0.000047       |
| 10000      | 2         | 12365   | 1.2365      | 3394.86  | 0.000732       |
| 100000     | 2         | -3676   | -0.03676    | 3344.19  | 0.00342        |
| 1000000    | 2         | -68318  | -0.068318   | 3364.42  | 0.046698       |
| 10000000   | 2         | -157791 | -0.0157791  | 3367.39  | 0.301161       |
| 100000000  | 2         | -283646 | -0.00283646 | 3366.82  | 4.68757        |
| 10         | 4         | 54      | 5.4         | 2412.41  | 0.00326        |
| 100        | 4         | 119     | 1.19        | 3566.33  | 0.002431       |
| 1000       | 4         | 1581    | 1.581       | 3434.23  | 0.001111       |
| 10000      | 4         | -1339   | -0.1339     | 3367.63  | 0.00637        |
| 100000     | 4         | 27784   | 0.27784     | 3375.97  | 0.003811       |
| 1000000    | 4         | 23818   | 0.023818    | 3365.79  | 0.047284       |
| 10000000   | 4         | 62424   | 0.0062424   | 3366.1   | 0.280694       |
| 100000000  | 4         | -361007 | -0.00361007 | 3366.64  | 2.23862        |
| 10         | 8         | 174     | 17.4        | 3017.89  | 0.002555       |
| 100        | 8         | 464     | 4.64        | 3519.61  | 0.011492       |
| 1000       | 8         | -643    | -0.643      | 3390.53  | 0.007649       |
| 10000      | 8         | -7620   | -0.762      | 3316.17  | 0.011642       |
| 100000     | 8         | -18242  | -0.18242    | 3364.48  | 0.119421       |
| 1000000    | 8         | 54925   | 0.054925    | 3368.84  | 0.089956       |
| 10000000   | 8         | -35695  | -0.0035695  | 3367.09  | 0.368936       |
| 100000000  | 8         | -192032 | -0.00192032 | 3366.26  | 2.59142        |

<img width="1358" height="618" alt="image" src="https://github.com/user-attachments/assets/ddfe6650-f903-45ee-a2f0-bf935ab0a22d" />

* **Strong scaling**: для фиксированного массива время выполнения уменьшается с увеличением числа процессов, но эффективность падает из-за накладных расходов на коммуникацию.
<img width="833" height="547" alt="image" src="https://github.com/user-attachments/assets/21a9f6b2-5727-497f-b080-2aa1436b407d" />
* **Weak scaling**: при увеличении размера массива вместе с числом процессов программа демонстрирует более стабильное время выполнения, так как вычислительная нагрузка на процесс сохраняется.
* Для маленьких массивов (<10⁴) использование большого числа процессов малоэффективно — накладные расходы MPI превышают выигрыш от параллельных вычислений.
* Для массивов ≥10⁶ элементов распределение нагрузки становится выгодным, но при масштабировании >32 процессов коммуникация начинает существенно замедлять программу.

#### **Выводы**

1. MPI-программа корректно вычисляет сумму, среднее и дисперсию распределённо.
2. Масштабируемость ограничена накладными расходами на коммуникацию (`MPI_Reduce`, `MPI_Allreduce`).
3. Strong scaling эффективен для больших массивов и умеренного числа процессов.
4. Weak scaling показывает, что при правильном распределении данных и объёме на процесс можно сохранять эффективность даже при увеличении числа процессов.
5. Для дальнейшей оптимизации можно использовать:

   * Non-blocking collectives (`MPI_Ireduce`, `MPI_Iallreduce`);
   * Дерево сокращения (tree-based reduction) для уменьшения времени коммуникации.


## Контрольные вопросы

Ответы на контрольные вопросы приведены в файле **questions.md**.


## Заключение

В ходе практической работы были:

* изучены методы профилирования CPU, GPU и распределённых программ;
* реализованы и оптимизированы параллельные и гибридные алгоритмы;
* проанализированы накладные расходы и узкие места;
* продемонстрировано влияние архитектуры и памяти на производительность.

Практическая работа дала целостное понимание **HPC-подходов и гибридных вычислений**, а также показала важность профилирования при разработке эффективных параллельных программ.

